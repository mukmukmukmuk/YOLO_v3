{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOYLVPJzo58X"
   },
   "source": [
    "# Library imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T05:25:28.889474Z",
     "iopub.status.busy": "2024-11-26T05:25:28.888759Z",
     "iopub.status.idle": "2024-11-26T05:25:28.894789Z",
     "shell.execute_reply": "2024-11-26T05:25:28.893715Z",
     "shell.execute_reply.started": "2024-11-26T05:25:28.889432Z"
    },
    "id": "hAptwiI0o58f",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:32.022777Z",
     "start_time": "2024-12-04T07:58:31.986221Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.ops import batched_nms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import display\n",
    "\n",
    "from torch.amp import GradScaler\n",
    "from torch.amp import autocast\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:32.140311Z",
     "start_time": "2024-12-04T07:58:32.079564Z"
    }
   },
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CHECKPOINT = \"dr_bee_checkpoint.pth.tar\"\n",
    "\n",
    "IMAGE_SIZE=640\n",
    "#ANCHORS는 image 단위에서 [0,1]의 값을 가짐\n",
    "ANCHORS = [\n",
    "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "]\n",
    "STRIDE=[32, 16, 8]\n",
    "GRID_SIZE=[ (IMAGE_SIZE//s) for s in STRIDE]\n",
    "SCALED_ANCHORS = (\n",
    "    torch.tensor(ANCHORS)\n",
    "    * torch.tensor(GRID_SIZE).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    ").to(DEVICE)\n",
    "\n",
    "#LEARNING HYPERPARAMETER\n",
    "EPOCHS=1\n",
    "BATCH_SIZE=2#16\n",
    "NUM_CLASSES = 3\n",
    "LEARNING_RATE=1e-4\n",
    "WEIGHT_DECAY=5e-4\n",
    "NMS_THRESHOLD=0.2\n",
    "CONFIDENCE_THRESHOLD=0.7\n",
    "\n",
    "#LOSS HYPERPARAMETER\n",
    "LAMBDA_COORD = 5.0\n",
    "LAMBDA_NO_OBJ = 0.5\n",
    "\n",
    "\n",
    "LOAD_MODEL=False\n",
    "SAVE_MODEL=True\n",
    "\n",
    "CLASS_LABEL={0:'normal',1:'mite',2:'virus'}\n",
    "\n",
    "#DIRECTORY\n",
    "IMG_TRAIN_DIR='./kaggle/input/dr-bee/images/train'\n",
    "LABEL_TRAIN_DIR='./kaggle/input/dr-bee/labels/train'\n",
    "\n",
    "IMG_TEST_DIR='./kaggle/input/dr-bee/images/val'\n",
    "LABEL_TEST_DIR='./kaggle/input/dr-bee/labels/val'\n",
    "\n",
    "OUTPUT_DIR='./kaggle/working/output_images'"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset define"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:36.089828Z",
     "start_time": "2024-12-04T07:58:35.942224Z"
    }
   },
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        #이미 image는 640x640크기에 검은 패딩이 추가 된 상태로 들어옴\n",
    "\n",
    "        # Random color jittering\n",
    "        A.ColorJitter(\n",
    "            brightness=0.5, contrast=0.5,\n",
    "            saturation=0.5, hue=0.5, p=0.5\n",
    "        ),\n",
    "        #0.5 확률로 수평 반전\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "\n",
    "        # 0.5 확률로 수직 반전\n",
    "        A.VerticalFlip(p=0.5),\n",
    "\n",
    "        #  # 랜덤 밝기 대비 조정\n",
    "        # A.RandomBrightnessContrast(\n",
    "        #     brightness_limit=0.2,\n",
    "        #     contrast_limit=0.2, p=0.5\n",
    "        # ),\n",
    "\n",
    "        # Normalize\n",
    "        A.Normalize(\n",
    "            mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255\n",
    "        ),\n",
    "        # Convert the image to PyTorch tensor\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    # Augmentation for bounding boxes\n",
    "    bbox_params=A.BboxParams(\n",
    "                    # yolo 형식은 x_center, y_center, widht, height로 넣어줘야함\n",
    "                    format=\"yolo\",\n",
    "                    #가시성이 떨어지는 애들은 제거\n",
    "                    min_visibility=0.4,\n",
    "                    label_fields=[]\n",
    "                )\n",
    ")\n",
    "\n",
    "# Transform for testing\n",
    "# 오류가 뜰 수 있지만 크게 상관 없음\n",
    "# 오류는 바운딩 박스를 처리하려는 옵션은 설정했지만, 이를 처리할 변환(transform)이 정의 되지 않아서 그런 것. 하지만 test data는 정규화 외에 어떤 작업도 하면 안됨\n",
    "test_transform = A.Compose(\n",
    "    [\n",
    "        #이미 image는 640x640크기에 검은 패딩이 추가 된 상태로 들어옴\n",
    "\n",
    "        # Normalize\n",
    "        A.Normalize(\n",
    "            mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255\n",
    "        ),\n",
    "        # Convert the image to PyTorch tensor\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    # Augmentation for bounding boxes\n",
    "    bbox_params=A.BboxParams(\n",
    "                    format=\"yolo\",\n",
    "                    min_visibility=0.4,\n",
    "                    label_fields=[]\n",
    "                )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:36.365688Z",
     "start_time": "2024-12-04T07:58:36.260398Z"
    }
   },
   "source": [
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, label_dir: str, img_dir: str, image_size: int = IMAGE_SIZE, anchors: list = ANCHORS,\n",
    "                 grid_size: list = GRID_SIZE, num_classes: int = NUM_CLASSES, transform=None):\n",
    "        self.label_dir = label_dir\n",
    "        self.img_dir = img_dir\n",
    "        self.image_size = image_size\n",
    "        self.grid_size = grid_size\n",
    "        self.num_classes = num_classes\n",
    "        # Anchor Box 크기, 이미 normalized됨 [(w1, h1), (w2, h2), ...]\n",
    "        self.anchors = anchors\n",
    "        self.transform = transform\n",
    "        self.image_ids = [os.path.splitext(os.path.basename(f))[0] for f in glob.glob(os.path.join(img_dir, '*'))\n",
    "                          if os.path.splitext(f)[1].lower() in ['.jpg', '.png', '.jpeg']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        label_path = os.path.join(self.label_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), shift=-1, axis=1).tolist()\n",
    "        img_path = os.path.join(self.img_dir, f\"{image_id}.jpg\")\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        bboxes = [([min(max(coord, 1e-6), 1.0) for coord in bbox[:4]] + [bbox[4]]) for bbox in bboxes]\n",
    "\n",
    "        if self.transform:\n",
    "            try:\n",
    "                transformed = self.transform(image=image, bboxes=bboxes)\n",
    "                image = transformed[\"image\"]\n",
    "                bboxes = transformed[\"bboxes\"]\n",
    "            except ValueError as e:\n",
    "                print(f\"{image_id} 이미지에서 {e} 발생\")\n",
    "                image = A.Compose([A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255),\n",
    "                                   ToTensorV2()])(image=image)[\"image\"]\n",
    "\n",
    "        targets = [torch.zeros((len(self.anchors[scale_idx]), size, size, 5 + self.num_classes)) for scale_idx, size in\n",
    "                   enumerate(self.grid_size)]\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            x_center, y_center, width, height, class_id = bbox\n",
    "            for scale_idx, grid_size in enumerate(self.grid_size):\n",
    "                grid_x = int(x_center * grid_size)\n",
    "                grid_y = int(y_center * grid_size)\n",
    "\n",
    "                anchor_boxes = torch.tensor(self.anchors[scale_idx], dtype=torch.float32)\n",
    "                box_wh = torch.tensor([width, height], dtype=torch.float32)\n",
    "                iou_scores = self.compute_iou(box_wh, anchor_boxes)\n",
    "                best_anchor = torch.argmax(iou_scores)\n",
    "\n",
    "                targets[scale_idx][best_anchor, grid_x, grid_y, 0] = 1  # Confidence\n",
    "                targets[scale_idx][best_anchor, grid_x, grid_y, 1:5] = torch.tensor([\n",
    "                    (x_center * grid_size) - grid_x,\n",
    "                    (y_center * grid_size) - grid_y,\n",
    "                    width * grid_size,\n",
    "                    height * grid_size\n",
    "                ])\n",
    "                targets[scale_idx][best_anchor, grid_x, grid_y, (5 + int(class_id))] = 1\n",
    "\n",
    "        return image, targets, image_id\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_iou(box_wh, anchor_boxes):\n",
    "        intersection = torch.clamp(\n",
    "            torch.min(box_wh[0], anchor_boxes[:, 0]) * torch.min(box_wh[1], anchor_boxes[:, 1]), min=0.0)\n",
    "        box_area = box_wh[0] * box_wh[1]\n",
    "        anchor_areas = anchor_boxes[:, 0] * anchor_boxes[:, 1]\n",
    "        union = box_area + anchor_areas - intersection\n",
    "        return intersection / union\n"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset TEST"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:38.053587Z",
     "start_time": "2024-12-04T07:58:38.014661Z"
    }
   },
   "source": [
    "# # 데이터셋 객체 생성\n",
    "# check_dataset = YoloDataset(\n",
    "#     label_dir=LABEL_TEST_DIR,\n",
    "#     img_dir=IMG_TEST_DIR,\n",
    "#     image_size=IMAGE_SIZE,\n",
    "#     anchors=ANCHORS,\n",
    "#     grid_size=GRID_SIZE,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     transform=test_transform\n",
    "# )\n",
    "#\n",
    "# # DataLoader 생성\n",
    "# #batch size가 1\n",
    "# check_dataset_loader = DataLoader(\n",
    "#     check_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True\n",
    "# )\n",
    "#\n",
    "# check_data_iter = iter(check_dataset_loader)\n",
    "# image, targets = next(check_data_iter)\n",
    "# # targets = [#scale,batch_size, #anchor, grid_size,grid_size,[...]]\n",
    "#\n",
    "# \"\"\"\n",
    "# 이미지와 바운딩 박스를 시각화\n",
    "# \"\"\"\n",
    "# fig, ax = plt.subplots(1)\n",
    "# #batch size가 있기 때문에, image[0]을 해서 1번째 batch의 img를 가져와야함\n",
    "# ax.imshow(image[0].permute(1, 2, 0))  # (C, H, W) -> (H, W, C)\n",
    "#\n",
    "# # 각 스케일에서 타겟 박스 그리기\n",
    "# print(\"3개의 scale에서 모두 동일한 bbox의 이미지가 뜨면 성공!\")\n",
    "# figures = []\n",
    "# for scale_idx in range(len(GRID_SIZE)):\n",
    "#     #batch size가 1이므로 [scale_idx][0] 인덱스를 불러옴\n",
    "#     target=targets[scale_idx][0]\n",
    "#     # 타겟에서 활성화된 박스만 가져오기\n",
    "#     active_boxes = torch.nonzero(target[..., 0])  # Confidence가 1인 경우\n",
    "#     for box in active_boxes:\n",
    "#         anchor_idx, grid_x, grid_y = box[:3]  # 앵커, 그리드 좌표 추출\n",
    "#\n",
    "#         cx, cy, w, h = target[anchor_idx, grid_x, grid_y, 1:5]  # 바운딩 박스 정보\n",
    "#\n",
    "#         # 실제 이미지 좌표로 변환\n",
    "#         cx = (grid_x + cx.item()) * STRIDE[scale_idx]\n",
    "#         cy = (grid_y + cy.item()) * STRIDE[scale_idx]\n",
    "#         w = w.item() * STRIDE[scale_idx]\n",
    "#         h = h.item() * STRIDE[scale_idx]\n",
    "#         # 바운딩 박스 그리기\n",
    "#         rect = patches.Rectangle(\n",
    "#             (cx - w / 2, cy - h / 2), w, h,\n",
    "#             linewidth=2, edgecolor=\"r\", facecolor=\"none\"\n",
    "#         )\n",
    "#         ax.add_patch(rect)\n",
    "#     figures.append(fig)\n",
    "# for fig in figures:\n",
    "#     display(fig)\n",
    "# plt.close()"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-p35Tn-mo58i"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8cdV3Ozo58j"
   },
   "source": [
    "## Blocks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YkP9x48go58k",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:42.216393Z",
     "start_time": "2024-12-04T07:58:42.081447Z"
    }
   },
   "source": [
    "# Basic Conv Block 정의\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, kernel_size:int, stride:int=1,padding:int=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # TODO : (Conv, BatchNorm, LeakyReLU) 스펙 보고 구현\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=False,stride=stride,padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# ResidualBlock 정의\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels:int, num_repeats:int=1): # residual block은 input channel 수와 output channel 수가 동일하다.\n",
    "        super().__init__()\n",
    "        res_layers=[]\n",
    "        for _ in range(num_repeats):\n",
    "            res_layers.append(nn.Sequential(\n",
    "            CNNBlock(in_channels,in_channels//2,kernel_size=1,stride=1,padding=0),\n",
    "            CNNBlock(in_channels//2,in_channels,kernel_size=3,stride=1,padding=1),\n",
    "        ))\n",
    "        self.layers = nn.ModuleList(res_layers)\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "        for layer in self.layers:\n",
    "            skip_connection = x\n",
    "            x = layer(x)\n",
    "            x+=skip_connection\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# DarkNet53 정의\n",
    "class Darknet53(nn.Module):\n",
    "    def __init__(self,in_channels:int=3):\n",
    "        super().__init__()\n",
    "        # TODO : define darknet53 (위에서 정의한 Conv block과 Res block 활용)\n",
    "        self.block1 = nn.Sequential(\n",
    "            CNNBlock(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            CNNBlock(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(64, num_repeats=1),\n",
    "            CNNBlock(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(128, num_repeats=2),\n",
    "            CNNBlock(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(256, num_repeats=8),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            CNNBlock(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(512, num_repeats=8),\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            CNNBlock(512, 1024, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(1024, num_repeats=4),\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "        # TODO : Darknet53에서 output으로 나오는 세가지 feature map 생산\n",
    "        high_feature_map = self.block1(x)\n",
    "        medium_feature_map = self.block2(high_feature_map)\n",
    "        low_feature_map = self.block3(medium_feature_map)\n",
    "        return high_feature_map, medium_feature_map, low_feature_map\n",
    "\n",
    "class UpSampling(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            # TODO : YOLO Network Architecture에서 Upsampling에 사용\n",
    "            CNNBlock(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "    def forward(self, x:torch.tensor):\n",
    "        return self.upsample(x)\n",
    "\n",
    "\n",
    "class YoloBlock(nn.Module):\n",
    "    def __init__(self,in_channels:int,out_channels:int):\n",
    "        super().__init__()\n",
    "        self.route_conv = nn.Sequential(\n",
    "            # TODO : define route conv & output conv\n",
    "            CNNBlock(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            CNNBlock(out_channels, out_channels*2, kernel_size=3, stride=1, padding=1),\n",
    "            CNNBlock(out_channels*2, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            CNNBlock(out_channels, out_channels*2, kernel_size=3, stride=1, padding=1),\n",
    "            CNNBlock(out_channels*2, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "        route = self.route_conv(x)\n",
    "        return route        #DetectionLayer로 전달\n",
    "\n",
    "\n",
    "class DetectionLayer(nn.Module):\n",
    "    def __init__(self, in_channels:int, num_classes:int):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # TODO : YOLO Network에서 output 된 결과를 이용하여 prediction\n",
    "\n",
    "        self.pred=nn.Sequential(\n",
    "            CNNBlock(in_channels, in_channels*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels*2,(num_classes+5)*3 , kernel_size=1,stride=1,padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "        output = self.pred(x)\n",
    "        # [batch size, (#bounding box) * (predicted bounding box), cell_width, cell_height ]\n",
    "        output = output.view(x.size(0), 3, self.num_classes + 5, x.size(2), x.size(3))\n",
    "        # [batch size, #bounding box,predicted bounding box, cell_width, cell_height ]\n",
    "        output = output.permute(0, 1, 3, 4, 2)\n",
    "        # [batch size, 동일한 scale에서의 #anchor box, grid_cell_size, grid_cell_size ,[confidence, grid_cell_x_center, grid_cell_y_center, grid_cell_width,grid_cell_height,P(class0), P(class1),P(class2)]]\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "te0Mc7A1o58m"
   },
   "source": [
    "## yolov3 architecture"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v31-1e26o58n",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:42.294573Z",
     "start_time": "2024-12-04T07:58:42.254254Z"
    }
   },
   "source": [
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels:int=3,num_classes:int= 3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.darknet = Darknet53(in_channels=in_channels)\n",
    "\n",
    "        self.yolo_block_01 = YoloBlock(1024,512)\n",
    "        self.detectlayer_01 = DetectionLayer(512, num_classes)\n",
    "        self.upsample_01 = UpSampling(512, 256)\n",
    "\n",
    "        # input_channels : darknet53 feature map 02 채널(512) + upsampling 채널(256)\n",
    "        self.yolo_block_02 = YoloBlock(512 + 256, 256)\n",
    "        self.detectlayer_02 = DetectionLayer(256, num_classes)\n",
    "        self.upsample_02 = UpSampling(256, 128)\n",
    "\n",
    "        # input_channels : darknet53 feature map 01 채널(256) + upsampling 채널(128)\n",
    "        self.yolo_block_03 = YoloBlock(256 + 128, 128)\n",
    "        self.detectlayer_03 = DetectionLayer(128, num_classes)\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "        high_feature_map, medium_feature_map, low_feature_map =self.darknet(x)\n",
    "\n",
    "        x= self.yolo_block_01(low_feature_map)\n",
    "        output_01 = self.detectlayer_01(x)\n",
    "        x = self.upsample_01(x)\n",
    "\n",
    "        x = self.yolo_block_02(torch.cat([x,medium_feature_map], dim=1))\n",
    "        output_02 = self.detectlayer_02(x)\n",
    "        x = self.upsample_02(x)\n",
    "\n",
    "        x = self.yolo_block_03(torch.cat([x, high_feature_map], dim=1))\n",
    "        output_03 = self.detectlayer_03(x)\n",
    "\n",
    "        return output_01, output_02, output_03"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture TEST"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:43.039129Z",
     "start_time": "2024-12-04T07:58:43.019836Z"
    }
   },
   "source": [
    "# # Creating model and testing output shapes\n",
    "# test_model = YOLOv3(num_classes=NUM_CLASSES)\n",
    "# x = torch.randn((1, 3, IMAGE_SIZE, IMAGE_SIZE))\n",
    "# out = test_model(x)\n",
    "# print(out[0].shape)\n",
    "# print(out[1].shape)\n",
    "# print(out[2].shape)\n",
    "#\n",
    "# # Asserting output shapes\n",
    "# assert test_model(x)[0].shape == (1, 3, IMAGE_SIZE // 32, IMAGE_SIZE // 32, NUM_CLASSES + 5) # B, RGB, cell size, cell size, (c, x, y, w, h) + classes_prob\n",
    "# assert test_model(x)[1].shape == (1, 3, IMAGE_SIZE // 16, IMAGE_SIZE // 16, NUM_CLASSES + 5)\n",
    "# assert test_model(x)[2].shape == (1, 3, IMAGE_SIZE // 8, IMAGE_SIZE // 8, NUM_CLASSES + 5)\n",
    "# print(\"Output shapes are correct!\")\n",
    "#\n",
    "# # torch summary\n",
    "# summary(test_model, input_size=(2, 3, IMAGE_SIZE, IMAGE_SIZE), device=DEVICE)"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np8KSXIBo58p"
   },
   "source": [
    "# Define Util & Loss function\n",
    "참고 자료 : https://www.geeksforgeeks.org/yolov3-from-scratch-using-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "st3cI3GOo58p"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:43.563116Z",
     "start_time": "2024-12-04T07:58:43.527998Z"
    }
   },
   "source": [
    "def giou(box1:torch.tensor, box2:torch.tensor)->torch.tensor:\n",
    "    # [x_center,y_center,width,height]\n",
    "    # batch를 고려하여 차원 유지\n",
    "    box1_x1 = (box1[..., 0:1] - box1[..., 2:3]) * 0.5\n",
    "    box1_y1 = (box1[..., 1:2] - box1[..., 3:4]) * 0.5\n",
    "    box1_x2 = (box1[..., 0:1] + box1[..., 2:3]) * 0.5\n",
    "    box1_y2 = (box1[..., 1:2] + box1[..., 3:4]) * 0.5\n",
    "\n",
    "    box2_x1 = (box2[..., 0:1] - box2[..., 2:3]) * 0.5\n",
    "    box2_y1 = (box2[..., 1:2] - box2[..., 3:4]) * 0.5\n",
    "    box2_x2 = (box2[..., 0:1] + box2[..., 2:3]) * 0.5\n",
    "    box2_y2 = (box2[..., 1:2] + box2[..., 3:4]) * 0.5\n",
    "\n",
    "    #intersection 계산\n",
    "    inter_x1 = torch.max(box1_x1, box2_x1)\n",
    "    inter_x2 = torch.min(box1_x2, box2_x2)\n",
    "    inter_y1 = torch.max(box1_y1, box2_y1)\n",
    "    inter_y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)\n",
    "    box1_area = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)\n",
    "    box2_area = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    iou = inter_area / torch.clamp(union_area, min=1e-6)\n",
    "\n",
    "    enclosing_x1 = torch.min(box1_x1, box2_x1)\n",
    "    enclosing_y1 = torch.min(box1_y1, box2_y1)\n",
    "    enclosing_x2 = torch.max(box1_x2, box2_x2)\n",
    "    enclosing_y2 = torch.max(box1_y2, box2_y2)\n",
    "    enclosing_area = torch.clamp(enclosing_x2 - enclosing_x1, min=0) * torch.clamp(enclosing_y2 - enclosing_y1, min=0)\n",
    "\n",
    "    giou = iou - (enclosing_area - union_area) / torch.clamp(enclosing_area, min=1e-6)\n",
    "\n",
    "    return giou"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e603RTh9o58s",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:44.013110Z",
     "start_time": "2024-12-04T07:58:43.989504Z"
    }
   },
   "source": [
    "def convert_cells_to_bboxes(pred :torch.tensor ,scaled_anchors:torch.tensor ):\n",
    "    #Scale은 벗겨진 상태로 들어옴\n",
    "    #[#batch,#anchor, grid_size, grid_size,[confidence, x, y, width, height, P(class1),P(class2),P(class3)]]\n",
    "    batch_size = pred.shape[0]\n",
    "    grid_size = pred.shape[2] #혹은 pred.shape[3]\n",
    "    num_anchors = len(scaled_anchors)\n",
    "    scaled_anchors = scaled_anchors.to(pred.device).reshape(1, num_anchors, 1, 1, 2)\n",
    "\n",
    "\n",
    "    box_predictions = pred[..., 1:5]\n",
    "\n",
    "    box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
    "    box_predictions[..., 2:4] = torch.exp(box_predictions[..., 2:4]) * scaled_anchors\n",
    "\n",
    "    pred_confidence = torch.sigmoid(pred[..., 0:1])\n",
    "    pred_best_class = torch.argmax(pred[..., 5:], dim=-1).unsqueeze(-1)\n",
    "\n",
    "    #0~(grid_size-1)의 값을 같는 tensor 생성\n",
    "    grid_indices = torch.arange(grid_size, device=pred.device).repeat(batch_size, num_anchors, grid_size, 1).unsqueeze(-1)\n",
    "\n",
    "    #grid_size로 나눠줘서 normalized 시킴\n",
    "    pred_x_center = (box_predictions[..., 0:1] + grid_indices)/grid_size\n",
    "    pred_y_center = (box_predictions[..., 1:2] + grid_indices.permute(0, 1, 3, 2, 4))/grid_size\n",
    "    pred_width_height = box_predictions[..., 2:4]/grid_size\n",
    "\n",
    "    converted_bboxes = torch.cat((pred_confidence, pred_x_center, pred_y_center, pred_width_height,pred_best_class), dim=-1)\n",
    "\n",
    "    # Grid Cell이 의미가 없으므로 중간을 풀어줌\n",
    "    converted_bboxes = converted_bboxes.reshape(batch_size, num_anchors * grid_size * grid_size, 6)\n",
    "\n",
    "    return converted_bboxes\n"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:44.338218Z",
     "start_time": "2024-12-04T07:58:44.325744Z"
    }
   },
   "source": [
    "def convert_coordinate_yolo_to_pascal(yolo_cordi:torch.tensor,img_size:int=IMAGE_SIZE):\n",
    "    \"\"\"\n",
    "    yolo_cordi의 formatting\n",
    "    [(#scale)*(#anchors)*grid_size*grid_size, [x_center,y_center,width,height]]\n",
    "    \"\"\"\n",
    "    img_scale_yolo_cordi = yolo_cordi[...,0:4]*IMAGE_SIZE\n",
    "    x_y_1= img_scale_yolo_cordi[...,0:2] - img_scale_yolo_cordi[...,2:4]/2\n",
    "    x_y_2= img_scale_yolo_cordi[...,0:2] + img_scale_yolo_cordi[...,2:4]/2\n",
    "    result_cordi=torch.cat((x_y_1,x_y_2),dim=-1)\n",
    "    return result_cordi"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1qCqWoRo58s"
   },
   "source": [
    "## Model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6En_Tp5co58t",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:44.679538Z",
     "start_time": "2024-12-04T07:58:44.655458Z"
    }
   },
   "source": [
    "def save_checkpoint(model, optimizer, filename = CHECKPOINT):\n",
    "    print(\"==> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "# Function to load checkpoint\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr=LEARNING_RATE, device=DEVICE):\n",
    "    print(\"==> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GWwaVWRo58t"
   },
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sME-_z6_o58t",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:44.987240Z",
     "start_time": "2024-12-04T07:58:44.963309Z"
    }
   },
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, lambda_coord:float = LAMBDA_COORD, lambda_no_obj:float=LAMBDA_NO_OBJ):\n",
    "        super().__init__()\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_no_obj = lambda_no_obj\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, pred:torch.tensor, target:torch.tensor, scaled_anchors:torch.tensor):\n",
    "        # Identifying which cells in target have objects and which have no objects\n",
    "        obj = target[..., 0] == 1\n",
    "        no_obj = target[..., 0] == 0\n",
    "\n",
    "        # Calculating No object loss\n",
    "        no_object_loss = self.bce(\n",
    "            (pred[..., 0:1][no_obj]), (target[..., 0:1][no_obj]),\n",
    "        )\n",
    "\n",
    "        # Reshaping anchors to match predictions\n",
    "        # bw, bh를 계산하기 위해서는 pw, ph 값이 필요하다.\n",
    "        num_anchors = len(scaled_anchors)\n",
    "        scaled_anchors = scaled_anchors.to(pred.device).reshape(1, num_anchors, 1, 1, 2)\n",
    "\n",
    "        # 모델을 통해서 Predict된 결과는 tx, ty, tw, th 이르모 bx, by, bw, bh fromat으로 맞춰준다.\n",
    "        # bx = sigmoid(tx) + cx 이지만 cx term을 여기서 더해줄 필요는 없다\n",
    "        # 이는 상대 좌표를 기반으로 손실 계산을 하기 때문이며 계산이 이미 그리드 셀 내 상대적 offset을 다루는데 초점이 맞춰져 있기 때문이다.\n",
    "        box_preds = torch.cat([self.sigmoid(pred[..., 1:3]),\n",
    "                               torch.exp(pred[..., 3:5]) * scaled_anchors], dim = -1)\n",
    "\n",
    "        # Calculating IoU for prediction and target\n",
    "        # iou_term = torchvision.ops.box_iou(box_preds[obj], target[..., 1:5][obj]).detach()\n",
    "\n",
    "        # torchvision.ops.box_iou는 box format을 (x1, y1, x2, y2)로 받기 때문에 (cx, cy, w, h)로 처리한 우리 tensor를 바로 넣을 수는 없다\n",
    "        # 해결방식 1. iou 직접 짜기\n",
    "        # 해결방식 2. formatting 해서 집어넣기\n",
    "        iou_term = giou(box_preds[obj], target[..., 1:5][obj]).detach()\n",
    "\n",
    "        # Calculating Object loss\n",
    "        # 논문에서는 object loss를 계산할 때 존재/비존재 모두 BCE를 쓴다고 나와있지만\n",
    "        # 실제로는 object가 존재할 경우 iou_term을 곱해주고 MSE 방식을 씀으로써\n",
    "        # 수렴 안정성, iou를 곱함으로써 객체의 정확한 위치와 겹침 정도 고려, 계산의 효율성을 챙길 수 있다.\n",
    "        # pred에 sigmoid를 씌우는 이유는 0과 1 사이 값으로 조절하기 위해서이다.\n",
    "        # target은 존재하는 경우 1로 고정이므로 iou_term을 곱해주면 0~1 사이의 값이 된다.\n",
    "        object_loss = self.mse(self.sigmoid(pred[..., 0:1][obj]),\n",
    "                               iou_term * target[..., 0:1][obj])\n",
    "\n",
    "        # Predicted box coordinates\n",
    "        # bx = sigmoid(tx)\n",
    "        pred[..., 1:3] = self.sigmoid(pred[..., 1:3])\n",
    "\n",
    "        # Target box coordinates\n",
    "        # bw = pw * exp(tw) 이므로 tw = log(pw / bw)\n",
    "        target[..., 3:5] = torch.log(torch.clamp(target[..., 3:5] / scaled_anchors, min=1e-6) )\n",
    "\n",
    "        # Calculating box coordinates\n",
    "        # 논문에서는 box coordinate loss를 계산할 때 'sum of squared error'를 사용한다고 나와있다.\n",
    "        # pred[...,1:5] format은 [tx, ty, tw, th]이고 target[..., 1:5] format은 [bx, by, bw, bh]이므로\n",
    "        # format을 통일 시켜줘야 한다.\n",
    "        # 논문에서는 target에 역함수를 취하여 pred[..., 1:5] format에 맞추어 계산한 것 같으나 위의 처리 후 아래 계산과 동일하다.\n",
    "        # 우리 format은 [bx, by, tw, th]로 맞춰주고 loss 계산\n",
    "        box_loss = self.mse(pred[..., 1:5][obj], target[..., 1:5][obj])\n",
    "\n",
    "        # Calculating class loss\n",
    "        # 논문에서는 아래와 같이 softmax(cross-entropy에서 사용됨)를 사용해보았을때 좋은 성능이 안나온다고 말했다.\n",
    "        # class_loss = self.ce((pred[..., 5:][obj]), target[..., 5:][obj].long())\n",
    "        # 논문을 따라가기 위해 우리도 logistic classifier(BCE에서 사용)을 사용해보자\n",
    "        class_loss = self.bce(pred[..., 5:][obj], target[..., 5:][obj].float())\n",
    "\n",
    "\n",
    "        return (\n",
    "            self.lambda_coord * box_loss\n",
    "            + object_loss\n",
    "            + self.lambda_no_obj * no_object_loss\n",
    "            + class_loss\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:45.246264Z",
     "start_time": "2024-12-04T07:58:45.228602Z"
    }
   },
   "source": [
    "def training_loop(loader, model, optimizer, loss_fn, scaler, scaled_anchors:torch.tensor=SCALED_ANCHORS):\n",
    "    # Creating a progress bar\n",
    "    # tqdm 통해서 진행도 표시\n",
    "    progress_bar = tqdm(loader, leave=True)\n",
    "\n",
    "    # Initializing a list to store the losses\n",
    "    # iteration 당 loss 값 기록\n",
    "    losses = []\n",
    "\n",
    "    # Iterating over the training data\n",
    "    for _, (images, targets, _) in enumerate(progress_bar):\n",
    "        # images는 이미지\n",
    "        # target은 grid_cell 단위의 label 데이터\n",
    "        # 이 두 개를 GPU나 CPU에 옮기기\n",
    "        images = images.to(DEVICE)\n",
    "        targets = [t.to(DEVICE) for t in targets]\n",
    "\n",
    "        # autocast()를 사용할 경우 모델이 더 빠르고 효율적으로 계산하도록 자동으로 숫자 크기를 줄여줌\n",
    "        with autocast(DEVICE):\n",
    "            # 만든 모델에 이미지 입력데이터(image)를 넣고 얻어낸 결과 <- grid_cell 단위, 크기 별로 pred[0], pred[1], pred[2] 존재\n",
    "            pred  = model(images)\n",
    "\n",
    "            # Calculating the loss at each scale\n",
    "            # 모델의 loss를 scale별로 계산하고 더하기\n",
    "            loss = 0\n",
    "            for i in range(3):\n",
    "                loss += loss_fn(pred[i], targets[i], scaled_anchors[i])\n",
    "\n",
    "        # Add the loss to the list\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \"\"\"\n",
    "        - 아래부터 사용되는 scaler는 torch.cuda.amp.GradScaler로\n",
    "        모델 훈련을 더 빠르고 효율적으로 하기 위해 사용\n",
    "        - 혼합 정밀도 문제를 해결\n",
    "        \"\"\"\n",
    "        # Backpropagate the loss\n",
    "        # 손실 값 스케일링을 통해서 float 16의 소수점 손실 문제 해결\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Optimization step\n",
    "        # 스케일링된 손실 값을 기반으로 모델 가중치 업데이트\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Update the scaler for next iteration\n",
    "        # 스케일링 값 조정\n",
    "        scaler.update()\n",
    "\n",
    "        # update progress bar with loss\n",
    "        # 손실 값들의 평균 계산\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        # 진행 바에 평균 손실 값 출력\n",
    "        progress_bar.set_postfix(loss=mean_loss)"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T07:58:45.547885Z",
     "start_time": "2024-12-04T07:58:45.537352Z"
    }
   },
   "source": [
    "# # 데이터셋 객체 생성\n",
    "# train_dataset = YoloDataset(\n",
    "#     label_dir=LABEL_TRAIN_DIR,\n",
    "#     img_dir=IMG_TRAIN_DIR,\n",
    "#     image_size=IMAGE_SIZE,\n",
    "#     anchors=ANCHORS,\n",
    "#     grid_size=GRID_SIZE,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     transform=train_transform\n",
    "# )\n",
    "#\n",
    "# # DataLoader 생성\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True\n",
    "# )\n",
    "#\n",
    "# model = YOLOv3(in_channels=3,num_classes=NUM_CLASSES).to(DEVICE)\n",
    "#\n",
    "# optimizer = optim.AdamW (model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "#\n",
    "# loss_fn = YoloLoss(LAMBDA_COORD,LAMBDA_NO_OBJ)\n",
    "# scaler = GradScaler()\n",
    "#\n",
    "# for e in range(1, EPOCHS+1):\n",
    "#     print(\"Epoch:\", e)\n",
    "#     training_loop(\n",
    "#         loader=train_loader,\n",
    "#         model=model,\n",
    "#         optimizer=optimizer,\n",
    "#         loss_fn=loss_fn,\n",
    "#         scaler=scaler,\n",
    "#         scaled_anchors=SCALED_ANCHORS\n",
    "#     )\n",
    "#\n",
    "#     # Saving the model\n",
    "#     if SAVE_MODEL:\n",
    "#         save_checkpoint(model, optimizer, filename=CHECKPOINT)"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T08:12:47.440147Z",
     "start_time": "2024-12-04T07:58:46.150635Z"
    }
   },
   "source": [
    "# Setting the load_model to True\n",
    "LOAD_MODEL = True\n",
    "\n",
    "# Defining the model, optimizer, loss function, and scaler\n",
    "model = YOLOv3(in_channels=3, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = YoloLoss()\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Loading the checkpoint if needed\n",
    "if LOAD_MODEL:\n",
    "    load_checkpoint(CHECKPOINT, model, optimizer, LEARNING_RATE)\n",
    "\n",
    "# Defining the test dataset and data loader\n",
    "test_dataset = YoloDataset(\n",
    "    label_dir=LABEL_TEST_DIR,\n",
    "    img_dir=IMG_TEST_DIR,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    anchors=ANCHORS,\n",
    "    grid_size=GRID_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    transform=test_transform,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "#BatchNorm에서 평가 모드와, 학습모드가 다름# Set the model to evaluation mode\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, targets, image_ids) in enumerate(test_loader):\n",
    "        # Move the input to the appropriate device\n",
    "        images = images.to(DEVICE)\n",
    "\n",
    "        # Getting the model predictions\n",
    "        preds = model(images)\n",
    "\n",
    "        # Getting bounding boxes for each scale\n",
    "        total_bboxes = []\n",
    "        for scale_idx in range(3):\n",
    "            boxes_by_scale = convert_cells_to_bboxes(preds[scale_idx], SCALED_ANCHORS[scale_idx])\n",
    "            total_bboxes.append(boxes_by_scale)\n",
    "\n",
    "        # Concatenate boxes across scales\n",
    "        total_bboxes = torch.cat(total_bboxes, dim=1)\n",
    "\n",
    "        for batch in range(images.shape[0]):\n",
    "            # Applying non-max suppression to remove overlapping bounding boxes\n",
    "            pascal_coords = convert_coordinate_yolo_to_pascal(total_bboxes[batch, ..., 1:5])\n",
    "            scores = total_bboxes[batch, ..., 0]\n",
    "            class_idxs = total_bboxes[batch, ..., 5]\n",
    "\n",
    "            nms_output_indices = batched_nms(\n",
    "                boxes=pascal_coords,\n",
    "                scores=scores,\n",
    "                idxs=class_idxs,\n",
    "                iou_threshold=NMS_THRESHOLD\n",
    "            )\n",
    "\n",
    "            # Filtering the boxes after NMS and applying confidence threshold\n",
    "            nms_output_bboxes = pascal_coords[nms_output_indices]\n",
    "            nms_scores = scores[nms_output_indices]\n",
    "            nms_class_idxs = class_idxs[nms_output_indices]\n",
    "\n",
    "            # Converting images for OpenCV\n",
    "            image_np = images[batch].permute(1, 2, 0).detach().cpu().numpy()  # Convert to [H, W, C]\n",
    "            image_np = (image_np * 255).astype(np.uint8)  # Convert to uint8 format (0-255 range)\n",
    "\n",
    "            # Ensure color channels are in BGR format for OpenCV\n",
    "            try:\n",
    "                if image_np.shape[-1] == 3:\n",
    "                    image_np = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "                else:\n",
    "                    print(f\"Unexpected number of channels in image: {image_np.shape[-1]}\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting color format: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Plotting bounding boxes on the image\n",
    "            for bbox, score, class_idx in zip(nms_output_bboxes, nms_scores, nms_class_idxs):\n",
    "                if score < CONFIDENCE_THRESHOLD:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, bbox)\n",
    "                if int(class_idx) == 0:\n",
    "                    color = (0, 255, 0)\n",
    "                elif int(class_idx) == 1:\n",
    "                    color = (0, 0, 255)\n",
    "                elif int(class_idx) == 2:\n",
    "                    color = (255, 0, 0)\n",
    "                else:\n",
    "                    color = (255, 255, 255)\n",
    "\n",
    "                cv2.rectangle(image_np, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "                text = f\"{CLASS_LABEL[int(class_idx)]}: {score:.2f}\"\n",
    "                cv2.putText(image_np, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "            # Save the image to /output_images directory with the original image ID in the filename\n",
    "            output_filename = f\"{image_ids[batch]}.jpg\"\n",
    "            output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "            # Attempt to save the image and provide detailed error information\n",
    "            try:\n",
    "                save_status = cv2.imwrite(output_path, image_np)\n",
    "                if not save_status:\n",
    "                    print(f\"Failed to save: {output_path}\")\n",
    "                    print(f\"Image shape: {image_np.shape}, dtype: {image_np.dtype}, path: {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving {output_path}: {e}\")\n",
    "\n",
    "# Set the model back to training mode\n",
    "model.train()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksm01\\AppData\\Local\\Temp\\ipykernel_26440\\1675466889.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "YOLOv3(\n",
       "  (darknet): Darknet53(\n",
       "    (block1): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): CNNBlock(\n",
       "              (conv): Sequential(\n",
       "                (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): LeakyReLU(negative_slope=0.1)\n",
       "              )\n",
       "            )\n",
       "            (1): CNNBlock(\n",
       "              (conv): Sequential(\n",
       "                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): LeakyReLU(negative_slope=0.1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Sequential(\n",
       "            (0): CNNBlock(\n",
       "              (conv): Sequential(\n",
       "                (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): LeakyReLU(negative_slope=0.1)\n",
       "              )\n",
       "            )\n",
       "            (1): CNNBlock(\n",
       "              (conv): Sequential(\n",
       "                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): LeakyReLU(negative_slope=0.1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): ResidualBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0-7): 8 x Sequential(\n",
       "            (0): CNNBlock(\n",
       "              (conv): Sequential(\n",
       "                (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): LeakyReLU(negative_slope=0.1)\n",
       "              )\n",
       "            )\n",
       "            (1): CNNBlock(\n",
       "              (conv): Sequential(\n",
       "                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): LeakyReLU(negative_slope=0.1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0-7): 8 x Sequential(\n",
       "            (0): CNNBlock(\n",
       "              (conv): Sequential(\n",
       "                (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): LeakyReLU(negative_slope=0.1)\n",
       "              )\n",
       "            )\n",
       "            (1): CNNBlock(\n",
       "              (conv): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): LeakyReLU(negative_slope=0.1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (block3): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x Sequential(\n",
       "            (0): CNNBlock(\n",
       "              (conv): Sequential(\n",
       "                (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): LeakyReLU(negative_slope=0.1)\n",
       "              )\n",
       "            )\n",
       "            (1): CNNBlock(\n",
       "              (conv): Sequential(\n",
       "                (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): LeakyReLU(negative_slope=0.1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (yolo_block_01): YoloBlock(\n",
       "    (route_conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (detectlayer_01): DetectionLayer(\n",
       "    (pred): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): Conv2d(1024, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (upsample_01): UpSampling(\n",
       "    (upsample): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    )\n",
       "  )\n",
       "  (yolo_block_02): YoloBlock(\n",
       "    (route_conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (detectlayer_02): DetectionLayer(\n",
       "    (pred): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (upsample_02): UpSampling(\n",
       "    (upsample): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    )\n",
       "  )\n",
       "  (yolo_block_03): YoloBlock(\n",
       "    (route_conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (detectlayer_03): DetectionLayer(\n",
       "    (pred): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Np8KSXIBo58p"
   ],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6134001,
     "sourceId": 10013715,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
