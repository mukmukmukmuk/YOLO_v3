{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 10013715,
     "sourceType": "datasetVersion",
     "datasetId": 6134001
    }
   ],
   "dockerImageVersionId": 30786,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "Np8KSXIBo58p"
   ]
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Library imports\n",
    "\n"
   ],
   "metadata": {
    "id": "SOYLVPJzo58X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-11-26T05:25:28.888759Z",
     "iopub.execute_input": "2024-11-26T05:25:28.889474Z",
     "iopub.status.idle": "2024-11-26T05:25:28.894789Z",
     "shell.execute_reply.started": "2024-11-26T05:25:28.889432Z",
     "shell.execute_reply": "2024-11-26T05:25:28.893715Z"
    },
    "id": "hAptwiI0o58f",
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:13.004572Z",
     "start_time": "2024-12-02T12:15:50.256567Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Config"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:13.041518Z",
     "start_time": "2024-12-02T12:16:13.030035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CHECKPOINT = \"checkpoint.pth.tar\"\n",
    "\n",
    "#ANCHORS는 image 단위에서 [0,1]의 값을 가짐\n",
    "ANCHORS = [\n",
    "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "]\n",
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE=1\n",
    "LEARNING_RATE=1e-5\n",
    "EPOCHS=20\n",
    "IMAGE_SIZE=640\n",
    "STRIDE=[32, 16, 8]\n",
    "WEIGHT_DECAY=0\n",
    "GRID_SIZE=[ (IMAGE_SIZE//s) for s in STRIDE]\n",
    "\n",
    "LOAD_MODEL=False\n",
    "SAVE_MODEL=True\n",
    "\n",
    "CLASS_LABEL=['normal','mite','virus']\n",
    "IMG_TRAIN_DIR='./kaggle/input/dr-bee/images/train'\n",
    "LABEL_TRAIN_DIR='./kaggle/input/dr-bee/labels/train'\n",
    "IMG_TEST_DIR='./kaggle/input/dr-bee/images/val'\n",
    "LABEL_TEST_DIR='./kaggle/input/dr-bee/labels/val'"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Architecture"
   ],
   "metadata": {
    "id": "-p35Tn-mo58i"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Blocks"
   ],
   "metadata": {
    "id": "J8cdV3Ozo58j"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Basic Conv Block 정의\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, kernel_size:int, stride:int=1,padding:int=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # TODO : (Conv, BatchNorm, LeakyReLU) 스펙 보고 구현\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=False,stride=stride,padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# ResidualBlock 정의\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels:int, num_repeats:int=1): # residual block은 input channel 수와 output channel 수가 동일하다.\n",
    "        super().__init__()\n",
    "        res_layers=[]\n",
    "        for _ in range(num_repeats):\n",
    "            res_layers.append(nn.Sequential(\n",
    "            CNNBlock(in_channels,in_channels//2,kernel_size=1,stride=1,padding=0),\n",
    "            CNNBlock(in_channels//2,in_channels,kernel_size=3,stride=1,padding=1),\n",
    "        ))\n",
    "        self.layers = nn.ModuleList(res_layers)\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "        for layer in self.layers:\n",
    "            skip_connection = x\n",
    "            x = layer(x)\n",
    "            x+=skip_connection\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# DarkNet53 정의\n",
    "class Darknet53(nn.Module):\n",
    "    def __init__(self,in_channels:int=3):\n",
    "        super().__init__()\n",
    "        # TODO : define darknet53 (위에서 정의한 Conv block과 Res block 활용)\n",
    "        self.block1 = nn.Sequential(\n",
    "            CNNBlock(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            CNNBlock(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(64, num_repeats=1),\n",
    "            CNNBlock(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(128, num_repeats=2),\n",
    "            CNNBlock(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(256, num_repeats=8),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            CNNBlock(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(512, num_repeats=8),\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            CNNBlock(512, 1024, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(1024, num_repeats=4),\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "        # TODO : Darknet53에서 output으로 나오는 세가지 feature map 생산\n",
    "        high_feature_map = self.block1(x)\n",
    "        medium_feature_map = self.block2(high_feature_map)\n",
    "        low_feature_map = self.block3(medium_feature_map)\n",
    "        return high_feature_map, medium_feature_map, low_feature_map\n",
    "\n",
    "class UpSampling(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            # TODO : YOLO Network Architecture에서 Upsampling에 사용\n",
    "            CNNBlock(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "    def forward(self, x:torch.tensor):\n",
    "        return self.upsample(x)\n",
    "\n",
    "\n",
    "class YoloBlock(nn.Module):\n",
    "    def __init__(self,in_channels:int,out_channels:int):\n",
    "        super().__init__()\n",
    "        self.route_conv = nn.Sequential(\n",
    "            # TODO : define route conv & output conv\n",
    "            CNNBlock(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            CNNBlock(out_channels, out_channels*2, kernel_size=3, stride=1, padding=1),\n",
    "            CNNBlock(out_channels*2, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            CNNBlock(out_channels, out_channels*2, kernel_size=3, stride=1, padding=1),\n",
    "            CNNBlock(out_channels*2, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "        route = self.route_conv(x)\n",
    "        return route        #DetectionLayer로 전달\n",
    "\n",
    "\n",
    "class DetectionLayer(nn.Module):\n",
    "    def __init__(self, in_channels:int, num_classes:int):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # TODO : YOLO Network에서 output 된 결과를 이용하여 prediction\n",
    "\n",
    "        self.pred=nn.Sequential(\n",
    "            CNNBlock(in_channels, in_channels*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels*2,(num_classes+5)*3 , kernel_size=1,stride=1,padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "        output = self.pred(x)\n",
    "        # [batch size, (#bounding box) * (predicted bounding box), cell_width, cell_height ]\n",
    "        output = output.view(x.size(0), 3, self.num_classes + 5, x.size(2), x.size(3))\n",
    "        # [batch size, #bounding box,predicted bounding box, cell_width, cell_height ]\n",
    "        output = output.permute(0, 1, 3, 4, 2)\n",
    "        # [batch size, 동일한 scale에서의 #anchor box, grid_cell_size, grid_cell_size ,[confidence, grid_cell_x_center, grid_cell_y_center, grid_cell_width,grid_cell_height,P(class0), P(class1),P(class2)]]\n",
    "        return output"
   ],
   "metadata": {
    "trusted": true,
    "id": "YkP9x48go58k",
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:13.889768Z",
     "start_time": "2024-12-02T12:16:13.864558Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## yolov3 architecture"
   ],
   "metadata": {
    "id": "te0Mc7A1o58m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels:int=3,num_classes:int= 3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.darknet = Darknet53(in_channels=in_channels)\n",
    "\n",
    "        self.yolo_block_01 = YoloBlock(1024,512)\n",
    "        self.detectlayer_01 = DetectionLayer(512, num_classes)\n",
    "        self.upsample_01 = UpSampling(512, 256)\n",
    "\n",
    "        # input_channels : darknet53 feature map 02 채널(512) + upsampling 채널(256)\n",
    "        self.yolo_block_02 = YoloBlock(512 + 256, 256)\n",
    "        self.detectlayer_02 = DetectionLayer(256, num_classes)\n",
    "        self.upsample_02 = UpSampling(256, 128)\n",
    "\n",
    "        # input_channels : darknet53 feature map 01 채널(256) + upsampling 채널(128)\n",
    "        self.yolo_block_03 = YoloBlock(256 + 128, 128)\n",
    "        self.detectlayer_03 = DetectionLayer(128, num_classes)\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "        high_feature_map, medium_feature_map, low_feature_map =self.darknet(x)\n",
    "\n",
    "        x= self.yolo_block_01(low_feature_map)\n",
    "        output_01 = self.detectlayer_01(x)\n",
    "        x = self.upsample_01(x)\n",
    "\n",
    "        x = self.yolo_block_02(torch.cat([x,medium_feature_map], dim=1))\n",
    "        output_02 = self.detectlayer_02(x)\n",
    "        x = self.upsample_02(x)\n",
    "\n",
    "        x = self.yolo_block_03(torch.cat([x, high_feature_map], dim=1))\n",
    "        output_03 = self.detectlayer_03(x)\n",
    "\n",
    "        return output_01, output_02, output_03"
   ],
   "metadata": {
    "trusted": true,
    "id": "v31-1e26o58n",
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:13.910659Z",
     "start_time": "2024-12-02T12:16:13.902780Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model test"
   ],
   "metadata": {
    "id": "79nbbOJZp0WK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# num_classes = 3\n",
    "#\n",
    "# # Creating model and testing output shapes\n",
    "# model = YOLOv3(num_classes=num_classes)\n",
    "# x = torch.randn((1, 3, IMAGE_SIZE, IMAGE_SIZE))\n",
    "# out = model(x)\n",
    "# print(out[0].shape)\n",
    "# print(out[1].shape)\n",
    "# print(out[2].shape)\n",
    "#\n",
    "# # Asserting output shapes\n",
    "# assert model(x)[0].shape == (1, 3, IMAGE_SIZE // 32, IMAGE_SIZE // 32, num_classes + 5) # B, RGB, cell size, cell size, (c, x, y, w, h) + classes_prob\n",
    "# assert model(x)[1].shape == (1, 3, IMAGE_SIZE // 16, IMAGE_SIZE // 16, num_classes + 5)\n",
    "# assert model(x)[2].shape == (1, 3, IMAGE_SIZE // 8, IMAGE_SIZE // 8, num_classes + 5)\n",
    "# print(\"Output shapes are correct!\")\n",
    "#\n",
    "# # torch summary\n",
    "# summary(model, input_size=(2, 3, IMAGE_SIZE, IMAGE_SIZE), device=\"cpu\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJdCBZjdprer",
    "outputId": "1007b764-4e97-4d71-be2b-b33a0ffa739f",
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:13.941578Z",
     "start_time": "2024-12-02T12:16:13.930Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Util & Loss function\n",
    "참고 자료 : https://www.geeksforgeeks.org/yolov3-from-scratch-using-pytorch/"
   ],
   "metadata": {
    "id": "Np8KSXIBo58p"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting anchors"
   ],
   "metadata": {
    "id": "st3cI3GOo58p"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### pytorch method 사용 예정"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:13.991663Z",
     "start_time": "2024-12-02T12:16:13.979351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Detection utils\n",
    "def giou(box1:torch.tensor, box2:torch.tensor)->torch.tensor:\n",
    "    # [x_center,y_center,width,height]\n",
    "    # batch를 고려하여 차원 유지\n",
    "    box1_x1 = (box1[..., 0:1] - box1[..., 2:3]) * 0.5\n",
    "    box1_y1 = (box1[..., 1:2] - box1[..., 3:4]) * 0.5\n",
    "    box1_x2 = (box1[..., 0:1] + box1[..., 2:3]) * 0.5\n",
    "    box1_y2 = (box1[..., 1:2] + box1[..., 3:4]) * 0.5\n",
    "\n",
    "    box2_x1 = (box2[..., 0:1] - box2[..., 2:3]) * 0.5\n",
    "    box2_y1 = (box2[..., 1:2] - box2[..., 3:4]) * 0.5\n",
    "    box2_x2 = (box2[..., 0:1] + box2[..., 2:3]) * 0.5\n",
    "    box2_y2 = (box2[..., 1:2] + box2[..., 3:4]) * 0.5\n",
    "\n",
    "    #intersection 계산\n",
    "    inter_x1 = torch.max(box1_x1, box2_x1)\n",
    "    inter_x2 = torch.min(box1_x2, box2_x2)\n",
    "    inter_y1 = torch.max(box1_y1, box2_y1)\n",
    "    inter_y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)\n",
    "    box1_area = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)\n",
    "    box2_area = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    _iou = inter_area / torch.clamp(union_area, min=1e-6)\n",
    "\n",
    "    enclosing_x1 = torch.min(box1_x1, box2_x1)\n",
    "    enclosing_y1 = torch.min(box1_y1, box2_y1)\n",
    "    enclosing_x2 = torch.max(box1_x2, box2_x2)\n",
    "    enclosing_y2 = torch.max(box1_y2, box2_y2)\n",
    "    enclosing_area = torch.clamp(enclosing_x2 - enclosing_x1, min=0) * torch.clamp(enclosing_y2 - enclosing_y1, min=0)\n",
    "\n",
    "    _giou = _iou - (enclosing_area - union_area) / torch.clamp(enclosing_area, min=1e-6)\n",
    "\n",
    "    return _giou"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "id": "wcgYhGCGo58r"
   },
   "cell_type": "markdown",
   "source": "## Detection utils"
  },
  {
   "cell_type": "code",
   "source": [
    "# def convert_cells_to_bboxes(pred :torch.tensor , anchors:list, stride :int,scale_num:int):\n",
    "#     # [#batch,#anchor, grid_size, grid_size,[confidence, x, y, width, height, P(class1),P(class2),P(class3)]]\n",
    "#     batch_size = pred.shape[0]\n",
    "#     num_anchors = len(anchors[scale_num])\n",
    "#     grid_size = pred.shape[2]\n",
    "#\n",
    "#     anchors = torch.tensor(anchors[scale_num], device=pred.device).reshape(1, num_anchors, 1, 1, 2)\n",
    "#\n",
    "#     box_predictions = pred[..., 1:5]\n",
    "#\n",
    "#     box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
    "#     box_predictions[..., 2:4] = torch.exp(box_predictions[..., 2:4]) * anchors\n",
    "#\n",
    "#     scores = torch.sigmoid(pred[..., 0:1])\n",
    "#     best_class = torch.argmax(pred[..., 5:], dim=-1).unsqueeze(-1)\n",
    "#\n",
    "#     grid_indices = torch.arange(grid_size, device=pred.device).repeat(batch_size, num_anchors, grid_size, 1).unsqueeze(-1)\n",
    "#\n",
    "#     x = (box_predictions[..., 0:1] + grid_indices) * stride\n",
    "#     y = (box_predictions[..., 1:2] + grid_indices.permute(0, 1, 3, 2, 4)) * stride\n",
    "#\n",
    "#     width_height = box_predictions[..., 2:4] * stride\n",
    "#\n",
    "#     converted_bboxes = torch.cat((scores, x, y, width_height,best_class), dim=-1)\n",
    "#     converted_bboxes = converted_bboxes.reshape(batch_size, num_anchors * grid_size * grid_size, 6)\n",
    "#\n",
    "#     return converted_bboxes\n"
   ],
   "metadata": {
    "trusted": true,
    "id": "e603RTh9o58s",
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:14.007085Z",
     "start_time": "2024-12-02T12:16:14.000678Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model checkpoint"
   ],
   "metadata": {
    "id": "n1qCqWoRo58s"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def save_checkpoint(model, optimizer, filename = \"dr_bee_checkpoint.ptr.tar\"):\n",
    "    print(\"==> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "# Function to load checkpoint\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr, device):\n",
    "    print(\"==> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ],
   "metadata": {
    "trusted": true,
    "id": "6En_Tp5co58t",
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:14.035387Z",
     "start_time": "2024-12-02T12:16:14.025832Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss function"
   ],
   "metadata": {
    "id": "1GWwaVWRo58t"
   }
  },
  {
   "metadata": {
    "trusted": true,
    "id": "sME-_z6_o58t",
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:14.084566Z",
     "start_time": "2024-12-02T12:16:14.065878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, lambda_coord:float = 5.0, lambda_no_obj:float=0.5):\n",
    "        super().__init__()\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_no_obj = lambda_no_obj\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, pred:torch.tensor, target:torch.tensor, anchors:list):\n",
    "        # Identifying which cells in target have objects and which have no objects\n",
    "        obj = target[..., 0] == 1\n",
    "        no_obj = target[..., 0] == 0\n",
    "\n",
    "        # Calculating No object loss\n",
    "        no_object_loss = self.bce(\n",
    "            (pred[..., 0:1][no_obj]), (target[..., 0:1][no_obj]),\n",
    "        )\n",
    "\n",
    "        # Reshaping anchors to match predictions\n",
    "        # bw, bh를 계산하기 위해서는 pw, ph 값이 필요하다.\n",
    "        num_anchors = len(anchors)\n",
    "        anchors = torch.tensor(anchors, device=pred.device).reshape(1, num_anchors, 1, 1, 2)\n",
    "\n",
    "        # 모델을 통해서 Predict된 결과는 tx, ty, tw, th 이르모 bx, by, bw, bh fromat으로 맞춰준다.\n",
    "        # bx = sigmoid(tx) + cx 이지만 cx term을 여기서 더해줄 필요는 없다\n",
    "        # 이는 상대 좌표를 기반으로 손실 계산을 하기 때문이며 계산이 이미 그리드 셀 내 상대적 offset을 다루는데 초점이 맞춰져 있기 때문이다.\n",
    "        box_preds = torch.cat([self.sigmoid(pred[..., 1:3]),\n",
    "                               torch.exp(pred[..., 3:5]) * anchors], dim = -1)\n",
    "\n",
    "        # Calculating IoU for prediction and target\n",
    "        # iou_term = torchvision.ops.box_iou(box_preds[obj], target[..., 1:5][obj]).detach()\n",
    "\n",
    "        # torchvision.ops.box_iou는 box format을 (x1, y1, x2, y2)로 받기 때문에 (cx, cy, w, h)로 처리한 우리 tensor를 바로 넣을 수는 없다\n",
    "        # 해결방식 1. iou 직접 짜기\n",
    "        # 해결방식 2. formatting 해서 집어넣기\n",
    "        iou_term = giou(box_preds[obj], target[..., 1:5][obj]).detach()\n",
    "\n",
    "        # Calculating Object loss\n",
    "        # 논문에서는 object loss를 계산할 때 존재/비존재 모두 BCE를 쓴다고 나와있지만\n",
    "        # 실제로는 object가 존재할 경우 iou_term을 곱해주고 MSE 방식을 씀으로써\n",
    "        # 수렴 안정성, iou를 곱함으로써 객체의 정확한 위치와 겹침 정도 고려, 계산의 효율성을 챙길 수 있다.\n",
    "        # pred에 sigmoid를 씌우는 이유는 0과 1 사이 값으로 조절하기 위해서이다.\n",
    "        # target은 존재하는 경우 1로 고정이므로 iou_term을 곱해주면 0~1 사이의 값이 된다.\n",
    "        object_loss = self.mse(self.sigmoid(pred[..., 0:1][obj]),\n",
    "                               iou_term * target[..., 0:1][obj])\n",
    "\n",
    "        # Predicted box coordinates\n",
    "        # bx = sigmoid(tx)\n",
    "        pred[..., 1:3] = self.sigmoid(pred[..., 1:3])\n",
    "\n",
    "        # Target box coordinates\n",
    "        # bw = pw * exp(tw) 이므로 tw = log(pw / bw)\n",
    "        target[..., 3:5] = torch.log(torch.clamp(target[..., 3:5] / anchors, min=1e-6) )\n",
    "\n",
    "        # Calculating box coordinates\n",
    "        # 논문에서는 box coordinate loss를 계산할 때 'sum of squared error'를 사용한다고 나와있다.\n",
    "        # pred[...,1:5] format은 [tx, ty, tw, th]이고 target[..., 1:5] format은 [bx, by, bw, bh]이므로\n",
    "        # format을 통일 시켜줘야 한다.\n",
    "        # 논문에서는 target에 역함수를 취하여 pred[..., 1:5] format에 맞추어 계산한 것 같으나 위의 처리 후 아래 계산과 동일하다.\n",
    "        # 우리 format은 [bx, by, tw, th]로 맞춰주고 loss 계산\n",
    "        box_loss = self.mse(pred[..., 1:5][obj], target[..., 1:5][obj])\n",
    "\n",
    "        # Calculating class loss\n",
    "        # 논문에서는 아래와 같이 softmax(cross-entropy에서 사용됨)를 사용해보았을때 좋은 성능이 안나온다고 말했다.\n",
    "        # class_loss = self.ce((pred[..., 5:][obj]), target[..., 5:][obj].long())\n",
    "        # 논문을 따라가기 위해 우리도 logistic classifier(BCE에서 사용)을 사용해보자\n",
    "        class_loss = self.bce(pred[..., 5:][obj], target[..., 5:][obj].float())\n",
    "\n",
    "\n",
    "        return (\n",
    "            self.lambda_coord * box_loss\n",
    "            + object_loss\n",
    "            + self.lambda_no_obj * no_object_loss\n",
    "            + class_loss\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:14.143066Z",
     "start_time": "2024-12-02T12:16:14.124967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def training_loop(loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
    "    # Creating a progress bar\n",
    "    # tqdm 통해서 진행도 표시\n",
    "    progress_bar = tqdm(loader, leave=True)\n",
    "\n",
    "    # Initializing a list to store the losses\n",
    "    # iteration 당 loss 값 기록\n",
    "    losses = []\n",
    "\n",
    "    # Iterating over the training data\n",
    "    for _, (images, targets) in enumerate(progress_bar):\n",
    "        # images는 이미지\n",
    "        # target은 grid_cell 단위의 label 데이터\n",
    "        # 이 두 개를 GPU나 CPU에 옮기기\n",
    "        images = images.to(DEVICE)\n",
    "        targets = [t.to(DEVICE) for t in targets]\n",
    "\n",
    "        # autocast()를 사용할 경우 모델이 더 빠르고 효율적으로 계산하도록 자동으로 숫자 크기를 줄여줌\n",
    "        with autocast():\n",
    "            # 만든 모델에 이미지 입력데이터(image)를 넣고 얻어낸 결과 <- grid_cell 단위, 크기 별로 pred[0], pred[1], pred[2] 존재\n",
    "            pred  = model(images)\n",
    "\n",
    "            # Calculating the loss at each scale\n",
    "            # 모델의 loss를 scale별로 계산하고 더하기\n",
    "            loss = 0\n",
    "            for i in range(3):\n",
    "                loss += loss_fn(pred[i], targets[i], scaled_anchors[i])\n",
    "\n",
    "        # Add the loss to the list\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \"\"\"\n",
    "        - 아래부터 사용되는 scaler는 torch.cuda.amp.GradScaler로\n",
    "        모델 훈련을 더 빠르고 효율적으로 하기 위해 사용\n",
    "        - 혼합 정밀도 문제를 해결\n",
    "        \"\"\"\n",
    "        # Backpropagate the loss\n",
    "        # 손실 값 스케일링을 통해서 float 16의 소수점 손실 문제 해결\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Optimization step\n",
    "        # 스케일링된 손실 값을 기반으로 모델 가중치 업데이트\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Update the scaler for next iteration\n",
    "        # 스케일링 값 조정\n",
    "        scaler.update()\n",
    "\n",
    "        # update progress bar with loss\n",
    "        # 손실 값들의 평균 계산\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        # 진행 바에 평균 손실 값 출력\n",
    "        progress_bar.set_postfix(loss=mean_loss)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Load"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:14.245637Z",
     "start_time": "2024-12-02T12:16:14.171145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        #이미 image는 640x640크기에 검은 패딩이 추가 된 상태로 들어옴\n",
    "\n",
    "        # Random color jittering\n",
    "        # A.ColorJitter(\n",
    "        #     brightness=0.5, contrast=0.5,\n",
    "        #     saturation=0.5, hue=0.5, p=0.5\n",
    "        # ),\n",
    "        # 0.5 확률로 수평 반전\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "\n",
    "        # 0.5 확률로 수직 반전\n",
    "        A.VerticalFlip(p=0.5),\n",
    "\n",
    "        #  # 랜덤 밝기 대비 조정\n",
    "        # A.RandomBrightnessContrast(\n",
    "        #     brightness_limit=0.2,\n",
    "        #     contrast_limit=0.2, p=0.5\n",
    "        # ),\n",
    "\n",
    "        # Normalize\n",
    "        A.Normalize(\n",
    "            mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255\n",
    "        ),\n",
    "        # Convert the image to PyTorch tensor\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    # Augmentation for bounding boxes\n",
    "    bbox_params=A.BboxParams(\n",
    "                    # yolo 형식은 x_center, y_center, widht, height로 넣어줘야함\n",
    "                    format=\"yolo\",\n",
    "                    #가시성이 떨어지는 애들은 제거\n",
    "                    min_visibility=0.4,\n",
    "                    label_fields=[]\n",
    "                )\n",
    ")\n",
    "\n",
    "# Transform for testing\n",
    "test_transform = A.Compose(\n",
    "    [\n",
    "        #이미 image는 640x640크기에 검은 패딩이 추가 된 상태로 들어옴\n",
    "\n",
    "        # Normalize\n",
    "        A.Normalize(\n",
    "            mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255\n",
    "        ),\n",
    "        # Convert the image to PyTorch tensor\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    # Augmentation for bounding boxes\n",
    "    bbox_params=A.BboxParams(\n",
    "                    format=\"yolo\",\n",
    "                    min_visibility=0.4,\n",
    "                    label_fields=[]\n",
    "                )\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\Lib\\site-packages\\albumentations\\core\\composition.py:243: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:14.339263Z",
     "start_time": "2024-12-02T12:16:14.310269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, label_dir:str, img_dir:str, image_size:int=IMAGE_SIZE,anchors:list=ANCHORS ,grid_size:list=GRID_SIZE,num_classes:int=NUM_CLASSES, transform=None):\n",
    "        self.label_dir = label_dir\n",
    "        self.img_dir = img_dir\n",
    "        self.image_size = image_size\n",
    "        self.grid_size = grid_size\n",
    "        self.num_classes = num_classes\n",
    "        # Anchor Box 크기, 이미 normalized됨 [(w1, h1), (w2, h2), ...]\n",
    "        self.anchors=anchors\n",
    "        self.transform = transform\n",
    "        self.image_ids = [os.path.splitext(os.path.basename(f))[0] for f in glob.glob(os.path.join(img_dir, '*'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        전체 label data의 개수\n",
    "        \"\"\"\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        CSV에서 이미지 이름 및 YOLO 형식의 데이터 가져오기\n",
    "        Image Unit -> Grid Cell Unit으로 저장해야함\n",
    "        \"\"\"\n",
    "        image_id = self.image_ids[idx]\n",
    "        label_path = os.path.join(self.label_dir, f\"{image_id}.txt\")\n",
    "\n",
    "        # txt 파일에서 바운딩 박스 및 클래스 정보 읽기\n",
    "        # np.loadtxt -> [class_id, x_center, y_center, width, height]\n",
    "        # np.roll -> [x_center, y_center, width, height, class_id]\n",
    "        # 이렇게 해야 transformed 할 때 알아서 잘 변형됨\n",
    "        bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2),shift=-1, axis=1).tolist()\n",
    "\n",
    "        # 이미지 로드\n",
    "        img_path = os.path.join(self.img_dir, f\"{image_id}.jpg\")\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        # 이미지 변환 적용\n",
    "        if self.transform:\n",
    "            transformed  = self.transform(image=image, bboxes=bboxes)\n",
    "            image = transformed[\"image\"]\n",
    "            bboxes = transformed[\"bboxes\"]\n",
    "\n",
    "        # target : [#scale * (#anchor, grid_cell_x, grid_cell_y, [confidence, x_center,y_center ,width, height, P(class0),P(class1),P(class2)])]\n",
    "        targets = [torch.zeros((len(self.anchors[scale_idx]), size, size, 5 +self.num_classes )) for scale_idx,size in enumerate(self.grid_size)]\n",
    "\n",
    "        # Bounding Box를 Grid Cell로 변환\n",
    "        for bbox in bboxes:\n",
    "            x_center, y_center, width, height,class_id = bbox\n",
    "            for scale_idx, grid_size in enumerate(self.grid_size):\n",
    "                # Grid Cell의 크기 (정규화된 상태에서 직접 사용)\n",
    "                grid_x = int(x_center * grid_size)  # x_center를 Grid Cell 인덱스로 변환\n",
    "                grid_y = int(y_center * grid_size)  # y_center를 Grid Cell 인덱스로 변환\n",
    "\n",
    "                # 앵커 박스와 IoU 계산\n",
    "                # anchor box와 bbox 모두 normalized 된 상태에서 계산 진행\n",
    "                anchor_boxes = torch.tensor(self.anchors[scale_idx], dtype=torch.float32)\n",
    "                box_wh = torch.tensor([width, height], dtype=torch.float32)\n",
    "                iou_scores = self.compute_iou(box_wh, anchor_boxes)\n",
    "                best_anchor = torch.argmax(iou_scores)  # IoU가 가장 높은 앵커 선택\n",
    "\n",
    "                \"\"\"\n",
    "                **예외**\n",
    "                같은 anchor box, grid_x, grid_y인 것이 나왔을 때, 덮어쓰기 될 가능성이 있음.\n",
    "\n",
    "                이것에 대한 예외처리는 따로 하지않음 -> 결국 이 상황은 다른 여러 개의 객체가 엄청 붙어있고, 그 상하좌우 비율도 동일하다는 건데, 과연 데이터를 만든 사람이 이렇게 겹쳐서 데이터를 만들 확률이 높을까 라는 생각이 듦. 그러니까 결국에 데이터를 수집할 때 처리를 잘 해주면 이런 문제는 발생하지 않는 다는 것이다!\n",
    "                \"\"\"\n",
    "\n",
    "                # Target 설정\n",
    "                targets[scale_idx][best_anchor, grid_x, grid_y, 0] = 1  # Confidence\n",
    "                targets[scale_idx][best_anchor, grid_x, grid_y, 1:5] = torch.tensor([\n",
    "                    (x_center * grid_size) - grid_x,  # x_center를 Grid Cell 내 상대 위치로 변환\n",
    "                    (y_center * grid_size) - grid_y,  # y_center를 Grid Cell 내 상대 위치로 변환\n",
    "                    width * grid_size,                 # width (grid cell 단위의 크기, normalized X)\n",
    "                    height * grid_size                # height (grid cell 단위의 크기, normalized X)\n",
    "                ])\n",
    "                targets[scale_idx][best_anchor, grid_x, grid_y, (5+ int(class_id))] = 1 # 클래스 레이블 원-핫 인코딩\n",
    "\n",
    "        return image, targets\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_iou(box_wh, anchor_boxes):\n",
    "        # IoU 계산\n",
    "        intersection = torch.clamp(torch.min(box_wh[0], anchor_boxes[:, 0]) * torch.min(box_wh[1], anchor_boxes[:, 1]), min=0.0)\n",
    "        box_area = box_wh[0] * box_wh[1]\n",
    "        anchor_areas = anchor_boxes[:, 0] * anchor_boxes[:, 1]\n",
    "        union = box_area + anchor_areas - intersection\n",
    "        return intersection / union"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:14.387563Z",
     "start_time": "2024-12-02T12:16:14.373286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터셋 객체 생성\n",
    "train_dataset = YoloDataset(\n",
    "    label_dir=LABEL_TRAIN_DIR,\n",
    "    img_dir=IMG_TRAIN_DIR,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    anchors=ANCHORS,\n",
    "    grid_size=GRID_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DATASET TEST CODE"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:14.459304Z",
     "start_time": "2024-12-02T12:16:14.447604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "# data_iter = iter(dataloader)\n",
    "# image, targets = next(data_iter)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:14.501522Z",
     "start_time": "2024-12-02T12:16:14.489948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def plot_image_with_boxes(image, targets, anchors, grid_size, batch_index:int=0,image_size=640,):\n",
    "#     \"\"\"\n",
    "#     이미지와 바운딩 박스를 시각화\n",
    "#     \"\"\"\n",
    "#     fig, ax = plt.subplots(1)\n",
    "#     ax.imshow(image.permute(1, 2, 0))  # (C, H, W) -> (H, W, C)\n",
    "#     # 각 스케일에서 타겟 박스 그리기\n",
    "#     for scale_idx in range(len(grid_size)):\n",
    "#         target=targets[scale_idx][batch_index]\n",
    "#         stride = image_size // grid_size[scale_idx]\n",
    "#\n",
    "#         # 타겟에서 활성화된 박스만 가져오기\n",
    "#         active_boxes = torch.nonzero(target[..., 0])  # Confidence가 1인 경우\n",
    "#         for box in active_boxes:\n",
    "#             anchor_idx, grid_x, grid_y = box[:3]  # 앵커, 그리드 좌표 추출\n",
    "#\n",
    "#             cx, cy, w, h = target[anchor_idx, grid_x, grid_y, 1:5]  # 바운딩 박스 정보\n",
    "#\n",
    "#             # 실제 이미지 좌표로 변환\n",
    "#             cx = (grid_x + cx.item()) * stride\n",
    "#             cy = (grid_y + cy.item()) * stride\n",
    "#             w = w.item() * stride\n",
    "#             h = h.item() * stride\n",
    "#\n",
    "#             print(cx, cy, w, h)\n",
    "#             # 바운딩 박스 그리기\n",
    "#             rect = patches.Rectangle(\n",
    "#                 (cx - w / 2, cy - h / 2), w, h,\n",
    "#                 linewidth=2, edgecolor=\"r\", facecolor=\"none\"\n",
    "#             )\n",
    "#             ax.add_patch(rect)\n",
    "#         plt.savefig(f\"plot_{scale_idx}.png\")\n",
    "#\n",
    "# # 이미지와 바운딩 박스 시각화\n",
    "# plot_image_with_boxes(image[0], targets, anchors=dataset.anchors, grid_size=dataset.grid_size)\n",
    "#\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T12:16:15.331830Z",
     "start_time": "2024-12-02T12:16:14.523823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = YOLOv3(in_channels=3,num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW (model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "loss_fn = YoloLoss()\n",
    "scaler = GradScaler()\n",
    "scaled_anchors = (\n",
    "    torch.tensor(ANCHORS)\n",
    "    * torch.tensor(STRIDE).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    ").to(DEVICE)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksm01\\AppData\\Local\\Temp\\ipykernel_9200\\852086308.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "E:\\anaconda3\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T12:20:46.834842Z",
     "start_time": "2024-12-02T12:16:15.446885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for e in range(1, EPOCHS+1):\n",
    "    print(\"Epoch:\", e)\n",
    "    training_loop(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
    "\n",
    "    # Saving the model\n",
    "    if SAVE_MODEL:\n",
    "        save_checkpoint(model, optimizer, filename=f\"dr_bee_checkpoint_{e}.pth.tar\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]C:\\Users\\ksm01\\AppData\\Local\\Temp\\ipykernel_9200\\3173498889.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "E:\\anaconda3\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "C:\\Users\\ksm01\\AppData\\Local\\Temp\\ipykernel_9200\\2250505040.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  anchors = torch.tensor(anchors, device=pred.device).reshape(1, num_anchors, 1, 1, 2)\n",
      "100%|██████████| 4/4 [00:42<00:00, 10.62s/it, loss=43]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving checkpoint\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:27<00:00,  6.78s/it, loss=41.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving checkpoint\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:26<00:00,  6.74s/it, loss=30.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving checkpoint\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:26<00:00,  6.51s/it, loss=39.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving checkpoint\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:30<00:00,  7.62s/it, loss=38]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving checkpoint\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:26<00:00,  6.62s/it, loss=28.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving checkpoint\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:27<00:00,  6.90s/it, loss=29.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving checkpoint\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:28<00:00,  7.06s/it, loss=26.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving checkpoint\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:22<00:07,  7.40s/it, loss=33]  \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TEST MODEL\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Setting the load_model to True\n",
    "# LOAD_MODEL = True\n",
    "#\n",
    "# # Defining the model, optimizer, loss function and scaler\n",
    "# model = YOLOv3(in_channels=3,num_classes=NUM_CLASSES).to(DEVICE)\n",
    "# optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE,weight_decay = WEIGHT_DECAY)\n",
    "# loss_fn = YoloLoss()\n",
    "# scaler = GradScaler()\n",
    "#\n",
    "# # Loading the checkpoint\n",
    "# if LOAD_MODEL:\n",
    "# \tload_checkpoint(f\"dr_bee_checkpoint_{EPOCHS}.pth.tar\", model, optimizer, LEARNING_RATE)\n",
    "#\n",
    "# # Defining the test dataset and data loader\n",
    "# test_dataset = YoloDataset(\n",
    "#     label_dir=LABEL_TEST_DIR,\n",
    "#     img_dir=IMG_TEST_DIR,\n",
    "#     image_size=IMAGE_SIZE,\n",
    "#     anchors=ANCHORS,\n",
    "#     grid_size=GRID_SIZE,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     transform=test_transform\n",
    "# )\n",
    "# test_loader = DataLoader(\n",
    "# \ttest_dataset,\n",
    "# \tbatch_size = BATCH_SIZE,\n",
    "# \tshuffle = True,\n",
    "# )\n",
    "#\n",
    "# # Getting a sample image from the test data loader\n",
    "# x, y = next(iter(test_loader))\n",
    "# x = x.to(DEVICE)\n",
    "#\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "# \t# Getting the model predictions\n",
    "# \toutput = model(x)\n",
    "# \t# Getting the bounding boxes from the predictions\n",
    "# \tbboxes = [[] for _ in range(x.shape[0])]\n",
    "#\n",
    "#\n",
    "# \t# Getting bounding boxes for each scale\n",
    "# \tfor i in range(3):\n",
    "# \t\tbatch_size, anchor_idx, grid_size, _, _ = output[i].shape\n",
    "# \t\tanchor = scaled_anchors[i]\n",
    "#         def convert_cells_to_bboxes(pred :torch.tensor , anchors:list, stride :int,scale_num=i):\n",
    "# \t\tboxes_scale_i = convert_cells_to_bboxes(\n",
    "# \t\t\t\t\t\t\toutput[i], anchor, s=S, is_predictions=True\n",
    "# \t\t\t\t\t\t)\n",
    "# \t\tfor idx, (box) in enumerate(boxes_scale_i):\n",
    "# \t\t\tbboxes[idx] += box\n",
    "# model.train()\n",
    "#\n",
    "# # Plotting the image with bounding boxes for each image in the batch\n",
    "# for i in range(batch_size):\n",
    "# \t# Applying non-max suppression to remove overlapping bounding boxes\n",
    "# \tnms_boxes = nms(bboxes[i], iou_threshold=0.5, threshold=0.6)\n",
    "# \t# Plotting the image with bounding boxes\n",
    "# \tplot_image(x[i].permute(1,2,0).detach().cpu(), nms_boxes)\n"
   ]
  }
 ]
}
