{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 10013715,
     "sourceType": "datasetVersion",
     "datasetId": 6134001
    }
   ],
   "dockerImageVersionId": 30786,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "Np8KSXIBo58p"
   ]
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Library imports\n",
    "\n"
   ],
   "metadata": {
    "id": "SOYLVPJzo58X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-11-26T05:25:28.888759Z",
     "iopub.execute_input": "2024-11-26T05:25:28.889474Z",
     "iopub.status.idle": "2024-11-26T05:25:28.894789Z",
     "shell.execute_reply.started": "2024-11-26T05:25:28.889432Z",
     "shell.execute_reply": "2024-11-26T05:25:28.893715Z"
    },
    "id": "hAptwiI0o58f",
    "ExecuteTime": {
     "end_time": "2024-11-28T01:21:49.195712Z",
     "start_time": "2024-11-28T01:21:35.528206Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Config"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CHECKPOINT = \"checkpoint.pth.tar\"\n",
    "\n",
    "ANCHORS = [\n",
    "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "]\n",
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE=32\n",
    "LEARNING_RATE=1e-5\n",
    "EPOCHS=20\n",
    "IMAGE_SIZE=640\n",
    "STRIDE=[32, 16, 8]\n",
    "WEIGHT_DECAY=0\n",
    "GRID_SIZE=[ (IMAGE_SIZE//s) for s in STRIDE]\n",
    "CLASS_LABEL=['normal','mite','virus']\n",
    "IMG_FOLDER_DIR='/kaggle/input/dr-bee/images/train'\n",
    "CSV_DIR='/kaggle/input/dr-bee/train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Architecture"
   ],
   "metadata": {
    "id": "-p35Tn-mo58i"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Blocks"
   ],
   "metadata": {
    "id": "J8cdV3Ozo58j"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Basic Conv Block 정의\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,padding=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # TODO : (Conv, BatchNorm, LeakyReLU) 스펙 보고 구현\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=False,stride=stride,padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# ResidualBlock 정의\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_repeats=1): # residual block은 input channel 수와 output channel 수가 동일하다.\n",
    "        super().__init__()\n",
    "        res_layers=[]\n",
    "        for _ in range(num_repeats):\n",
    "            res_layers.append(nn.Sequential(\n",
    "            CNNBlock(in_channels,in_channels//2,kernel_size=1,stride=1,padding=0),\n",
    "            CNNBlock(in_channels//2,in_channels,kernel_size=3,stride=1,padding=1),\n",
    "        ))\n",
    "        self.layers = nn.ModuleList(res_layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            skip_connection = x\n",
    "            x = layer(x)\n",
    "            x+=skip_connection\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# DarkNet53 정의\n",
    "class Darknet53(nn.Module):\n",
    "    def __init__(self,in_channels=3):\n",
    "        super().__init__()\n",
    "        # TODO : define darknet53 (위에서 정의한 Conv block과 Res block 활용)\n",
    "        self.block1 = nn.Sequential(\n",
    "            CNNBlock(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            CNNBlock(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(64, num_repeats=1),\n",
    "            CNNBlock(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(128, num_repeats=2),\n",
    "            CNNBlock(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(256, num_repeats=8),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            CNNBlock(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(512, num_repeats=8),\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            CNNBlock(512, 1024, kernel_size=3, stride=2, padding=1),\n",
    "            ResidualBlock(1024, num_repeats=4),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO : Darknet53에서 output으로 나오는 세가지 feature map 생산\n",
    "        high_feature_map = self.block1(x)\n",
    "        medium_feature_map = self.block2(high_feature_map)\n",
    "        low_feature_map = self.block3(medium_feature_map)\n",
    "        return high_feature_map, medium_feature_map, low_feature_map\n",
    "\n",
    "class UpSampling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            # TODO : YOLO Network Architecture에서 Upsampling에 사용\n",
    "            CNNBlock(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)\n",
    "\n",
    "\n",
    "class YoloBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels):\n",
    "        super().__init__()\n",
    "        self.route_conv = nn.Sequential(\n",
    "            # TODO : define route conv & output conv\n",
    "            CNNBlock(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            CNNBlock(out_channels, out_channels*2, kernel_size=3, stride=1, padding=1),\n",
    "            CNNBlock(out_channels*2, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            CNNBlock(out_channels, out_channels*2, kernel_size=3, stride=1, padding=1),\n",
    "            CNNBlock(out_channels*2, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        route = self.route_conv(x)\n",
    "        return route        #DetectionLayer로 전달\n",
    "\n",
    "\n",
    "class DetectionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # TODO : YOLO Network에서 output 된 결과를 이용하여 prediction\n",
    "\n",
    "        self.pred=nn.Sequential(\n",
    "            CNNBlock(in_channels, in_channels*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels*2,(num_classes+5)*3 , kernel_size=1,stride=1,padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pred(x)\n",
    "        # [batch size, (#bounding box) * (predicted bounding box), cell_width, cell_height ]\n",
    "        output = output.view(x.size(0), 3, self.num_classes + 5, x.size(2), x.size(3))\n",
    "        # [batch size, #bounding box,predicted bounding box, cell_width, cell_height ]\n",
    "        output = output.permute(0, 1, 3, 4, 2)\n",
    "        # [batch size, #bounding box, cell_width, cell_height ,predicted bounding box]\n",
    "        return output"
   ],
   "metadata": {
    "trusted": true,
    "id": "YkP9x48go58k",
    "ExecuteTime": {
     "end_time": "2024-11-28T01:21:49.256181Z",
     "start_time": "2024-11-28T01:21:49.222796Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## yolov3 architecture"
   ],
   "metadata": {
    "id": "te0Mc7A1o58m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels=3,num_classes = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.darknet = Darknet53(in_channels=in_channels)\n",
    "\n",
    "        self.yolo_block_01 = YoloBlock(1024,512)\n",
    "        self.detectlayer_01 = DetectionLayer(512, num_classes)\n",
    "        self.upsample_01 = UpSampling(512, 256)\n",
    "\n",
    "        # input_channels : darknet53 feature map 02 채널(512) + upsampling 채널(256)\n",
    "        self.yolo_block_02 = YoloBlock(512 + 256, 256)\n",
    "        self.detectlayer_02 = DetectionLayer(256, num_classes)\n",
    "        self.upsample_02 = UpSampling(256, 128)\n",
    "\n",
    "        # input_channels : darknet53 feature map 01 채널(256) + upsampling 채널(128)\n",
    "        self.yolo_block_03 = YoloBlock(256 + 128, 128)\n",
    "        self.detectlayer_03 = DetectionLayer(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        high_feature_map, medium_feature_map, low_feature_map =self.darknet(x)\n",
    "\n",
    "        x= self.yolo_block_01(low_feature_map)\n",
    "        output_01 = self.detectlayer_01(x)\n",
    "        x = self.upsample_01(x)\n",
    "\n",
    "        x = self.yolo_block_02(torch.cat([x,medium_feature_map], dim=1))\n",
    "        output_02 = self.detectlayer_02(x)\n",
    "        x = self.upsample_02(x)\n",
    "\n",
    "        x = self.yolo_block_03(torch.cat([x, high_feature_map], dim=1))\n",
    "        output_03 = self.detectlayer_03(x)\n",
    "\n",
    "        return output_01, output_02, output_03"
   ],
   "metadata": {
    "trusted": true,
    "id": "v31-1e26o58n",
    "ExecuteTime": {
     "end_time": "2024-11-28T01:21:49.969962Z",
     "start_time": "2024-11-28T01:21:49.951549Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model test"
   ],
   "metadata": {
    "id": "79nbbOJZp0WK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# num_classes = 3\n",
    "#\n",
    "# # Creating model and testing output shapes\n",
    "# model = YOLOv3(num_classes=num_classes)\n",
    "# x = torch.randn((1, 3, IMAGE_SIZE, IMAGE_SIZE))\n",
    "# out = model(x)\n",
    "# print(out[0].shape)\n",
    "# print(out[1].shape)\n",
    "# print(out[2].shape)\n",
    "#\n",
    "# # Asserting output shapes\n",
    "# assert model(x)[0].shape == (1, 3, IMAGE_SIZE // 32, IMAGE_SIZE // 32, num_classes + 5) # B, RGB, cell size, cell size, (c, x, y, w, h) + classes_prob\n",
    "# assert model(x)[1].shape == (1, 3, IMAGE_SIZE // 16, IMAGE_SIZE // 16, num_classes + 5)\n",
    "# assert model(x)[2].shape == (1, 3, IMAGE_SIZE // 8, IMAGE_SIZE // 8, num_classes + 5)\n",
    "# print(\"Output shapes are correct!\")\n",
    "#\n",
    "# # torch summary\n",
    "# summary(model, input_size=(2, 3, IMAGE_SIZE, IMAGE_SIZE), device=\"cpu\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJdCBZjdprer",
    "outputId": "1007b764-4e97-4d71-be2b-b33a0ffa739f",
    "ExecuteTime": {
     "end_time": "2024-11-28T01:24:03.960727Z",
     "start_time": "2024-11-28T01:23:56.572778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 13, 13, 8])\n",
      "torch.Size([1, 3, 26, 26, 8])\n",
      "torch.Size([1, 3, 52, 52, 8])\n",
      "Output shapes are correct!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "YOLOv3                                                       [2, 3, 13, 13, 8]         --\n",
       "├─Darknet53: 1-1                                             [2, 256, 52, 52]          --\n",
       "│    └─Sequential: 2-1                                       [2, 256, 52, 52]          --\n",
       "│    │    └─CNNBlock: 3-1                                    [2, 32, 416, 416]         928\n",
       "│    │    └─CNNBlock: 3-2                                    [2, 64, 208, 208]         18,560\n",
       "│    │    └─ResidualBlock: 3-3                               [2, 64, 208, 208]         20,672\n",
       "│    │    └─CNNBlock: 3-4                                    [2, 128, 104, 104]        73,984\n",
       "│    │    └─ResidualBlock: 3-5                               [2, 128, 104, 104]        164,608\n",
       "│    │    └─CNNBlock: 3-6                                    [2, 256, 52, 52]          295,424\n",
       "│    │    └─ResidualBlock: 3-7                               [2, 256, 52, 52]          2,627,584\n",
       "│    └─Sequential: 2-2                                       [2, 512, 26, 26]          --\n",
       "│    │    └─CNNBlock: 3-8                                    [2, 512, 26, 26]          1,180,672\n",
       "│    │    └─ResidualBlock: 3-9                               [2, 512, 26, 26]          10,498,048\n",
       "│    └─Sequential: 2-3                                       [2, 1024, 13, 13]         --\n",
       "│    │    └─CNNBlock: 3-10                                   [2, 1024, 13, 13]         4,720,640\n",
       "│    │    └─ResidualBlock: 3-11                              [2, 1024, 13, 13]         20,983,808\n",
       "├─YoloBlock: 1-2                                             [2, 512, 13, 13]          --\n",
       "│    └─Sequential: 2-4                                       [2, 512, 13, 13]          --\n",
       "│    │    └─CNNBlock: 3-12                                   [2, 512, 13, 13]          525,312\n",
       "│    │    └─CNNBlock: 3-13                                   [2, 1024, 13, 13]         4,720,640\n",
       "│    │    └─CNNBlock: 3-14                                   [2, 512, 13, 13]          525,312\n",
       "│    │    └─CNNBlock: 3-15                                   [2, 1024, 13, 13]         4,720,640\n",
       "│    │    └─CNNBlock: 3-16                                   [2, 512, 13, 13]          525,312\n",
       "├─DetectionLayer: 1-3                                        [2, 3, 13, 13, 8]         --\n",
       "│    └─Sequential: 2-5                                       [2, 24, 13, 13]           --\n",
       "│    │    └─CNNBlock: 3-17                                   [2, 1024, 13, 13]         4,720,640\n",
       "│    │    └─Conv2d: 3-18                                     [2, 24, 13, 13]           24,600\n",
       "├─UpSampling: 1-4                                            [2, 256, 26, 26]          --\n",
       "│    └─Sequential: 2-6                                       [2, 256, 26, 26]          --\n",
       "│    │    └─CNNBlock: 3-19                                   [2, 256, 13, 13]          131,584\n",
       "│    │    └─Upsample: 3-20                                   [2, 256, 26, 26]          --\n",
       "├─YoloBlock: 1-5                                             [2, 256, 26, 26]          --\n",
       "│    └─Sequential: 2-7                                       [2, 256, 26, 26]          --\n",
       "│    │    └─CNNBlock: 3-21                                   [2, 256, 26, 26]          197,120\n",
       "│    │    └─CNNBlock: 3-22                                   [2, 512, 26, 26]          1,180,672\n",
       "│    │    └─CNNBlock: 3-23                                   [2, 256, 26, 26]          131,584\n",
       "│    │    └─CNNBlock: 3-24                                   [2, 512, 26, 26]          1,180,672\n",
       "│    │    └─CNNBlock: 3-25                                   [2, 256, 26, 26]          131,584\n",
       "├─DetectionLayer: 1-6                                        [2, 3, 26, 26, 8]         --\n",
       "│    └─Sequential: 2-8                                       [2, 24, 26, 26]           --\n",
       "│    │    └─CNNBlock: 3-26                                   [2, 512, 26, 26]          1,180,672\n",
       "│    │    └─Conv2d: 3-27                                     [2, 24, 26, 26]           12,312\n",
       "├─UpSampling: 1-7                                            [2, 128, 52, 52]          --\n",
       "│    └─Sequential: 2-9                                       [2, 128, 52, 52]          --\n",
       "│    │    └─CNNBlock: 3-28                                   [2, 128, 26, 26]          33,024\n",
       "│    │    └─Upsample: 3-29                                   [2, 128, 52, 52]          --\n",
       "├─YoloBlock: 1-8                                             [2, 128, 52, 52]          --\n",
       "│    └─Sequential: 2-10                                      [2, 128, 52, 52]          --\n",
       "│    │    └─CNNBlock: 3-30                                   [2, 128, 52, 52]          49,408\n",
       "│    │    └─CNNBlock: 3-31                                   [2, 256, 52, 52]          295,424\n",
       "│    │    └─CNNBlock: 3-32                                   [2, 128, 52, 52]          33,024\n",
       "│    │    └─CNNBlock: 3-33                                   [2, 256, 52, 52]          295,424\n",
       "│    │    └─CNNBlock: 3-34                                   [2, 128, 52, 52]          33,024\n",
       "├─DetectionLayer: 1-9                                        [2, 3, 52, 52, 8]         --\n",
       "│    └─Sequential: 2-11                                      [2, 24, 52, 52]           --\n",
       "│    │    └─CNNBlock: 3-35                                   [2, 256, 52, 52]          295,424\n",
       "│    │    └─Conv2d: 3-36                                     [2, 24, 52, 52]           6,168\n",
       "==============================================================================================================\n",
       "Total params: 61,534,504\n",
       "Trainable params: 61,534,504\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 65.30\n",
       "==============================================================================================================\n",
       "Input size (MB): 4.15\n",
       "Forward/backward pass size (MB): 1226.60\n",
       "Params size (MB): 246.14\n",
       "Estimated Total Size (MB): 1476.89\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Util & Loss function\n",
    "참고 자료 : https://www.geeksforgeeks.org/yolov3-from-scratch-using-pytorch/"
   ],
   "metadata": {
    "id": "Np8KSXIBo58p"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting anchors"
   ],
   "metadata": {
    "id": "st3cI3GOo58p"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### pytorch method 사용 예정"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Detection utils\n",
    "# def Giou(box1:torch.tensor, box2:torch.tensor):\n",
    "#     # [x_center,y_center,width,height]\n",
    "#     # batch를 고려하여 차원 유지\n",
    "#     box1_x1 = (box1[..., 0:1] - box1[..., 2:3]) * 0.5\n",
    "#     box1_y1 = (box1[..., 1:2] - box1[..., 3:4]) * 0.5\n",
    "#     box1_x2 = (box1[..., 0:1] + box1[..., 2:3]) * 0.5\n",
    "#     box1_y2 = (box1[..., 1:2] + box1[..., 3:4]) * 0.5\n",
    "#\n",
    "#     box2_x1 = (box2[..., 0:1] - box2[..., 2:3]) * 0.5\n",
    "#     box2_y1 = (box2[..., 1:2] - box2[..., 3:4]) * 0.5\n",
    "#     box2_x2 = (box2[..., 0:1] + box2[..., 2:3]) * 0.5\n",
    "#     box2_y2 = (box2[..., 1:2] + box2[..., 3:4]) * 0.5\n",
    "#\n",
    "#     #intersection 계산\n",
    "#     inter_x1 = torch.max(box1_x1, box2_x1)\n",
    "#     inter_x2 = torch.min(box1_x2, box2_x2)\n",
    "#     inter_y1 = torch.max(box1_y1, box2_y1)\n",
    "#     inter_y2 = torch.min(box1_y2, box2_y2)\n",
    "#\n",
    "#     inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)\n",
    "#     box1_area = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)\n",
    "#     box2_area = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)\n",
    "#     union_area = box1_area + box2_area - inter_area\n",
    "#\n",
    "#     iou = inter_area / torch.clamp(union_area, min=1e-6)\n",
    "#\n",
    "#     enclosing_x1 = torch.min(box1_x1, box2_x1)\n",
    "#     enclosing_y1 = torch.min(box1_y1, box2_y1)\n",
    "#     enclosing_x2 = torch.max(box1_x2, box2_x2)\n",
    "#     enclosing_y2 = torch.max(box1_y2, box2_y2)\n",
    "#     enclosing_area = torch.clamp(enclosing_x2 - enclosing_x1, min=0) * torch.clamp(enclosing_y2 - enclosing_y1, min=0)\n",
    "#\n",
    "#     giou = iou - (enclosing_area - union_area) / torch.clamp(enclosing_area, min=1e-6)\n",
    "#\n",
    "#     return giou"
   ]
  },
  {
   "metadata": {
    "id": "wcgYhGCGo58r"
   },
   "cell_type": "markdown",
   "source": "## Detection utils"
  },
  {
   "cell_type": "code",
   "source": [
    "def convert_cells_to_bboxes(predictions :torch.tensor , anchors:list, stride :int):\n",
    "    # [confidence, x, y, width, height, P(class1),P(class2),P(class3)]\n",
    "    batch_size = predictions.shape[0]\n",
    "    num_anchors = len(anchors)\n",
    "    grid_size = predictions.shape[2]\n",
    "\n",
    "    anchors = torch.tensor(anchors, device=predictions.device).reshape(1, num_anchors, 1, 1, 2)\n",
    "\n",
    "    box_predictions = predictions[..., 1:5]\n",
    "\n",
    "    box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
    "    box_predictions[..., 2:4] = torch.exp(box_predictions[..., 2:4]) * anchors\n",
    "\n",
    "    scores = torch.sigmoid(predictions[..., 0:1])\n",
    "    best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n",
    "\n",
    "    grid_indices = torch.arange(grid_size, device=predictions.device).repeat(batch_size, num_anchors, grid_size, 1).unsqueeze(-1)\n",
    "\n",
    "    x = (box_predictions[..., 0:1] + grid_indices) * stride\n",
    "    y = (box_predictions[..., 1:2] + grid_indices.permute(0, 1, 3, 2, 4)) * stride\n",
    "\n",
    "    width_height = box_predictions[..., 2:4] * stride\n",
    "\n",
    "    converted_bboxes = torch.cat((scores, x, y, width_height,best_class), dim=-1)\n",
    "    converted_bboxes = converted_bboxes.reshape(batch_size, num_anchors * grid_size * grid_size, 6)\n",
    "\n",
    "    return converted_bboxes\n",
    "\n",
    "def plot_image(image, boxes):\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "id": "e603RTh9o58s",
    "ExecuteTime": {
     "end_time": "2024-11-28T01:22:02.224857Z",
     "start_time": "2024-11-28T01:22:02.212538Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model checkpoint"
   ],
   "metadata": {
    "id": "n1qCqWoRo58s"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def save_checkpoint(model, optimizer, filename = \"dr_bee_checkpoint.ptr.tar\"):\n",
    "    print(\"==> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "# Function to load checkpoint\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr, device):\n",
    "    print(\"==> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "id": "6En_Tp5co58t",
    "ExecuteTime": {
     "end_time": "2024-11-28T01:22:02.363808Z",
     "start_time": "2024-11-28T01:22:02.350365Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss function"
   ],
   "metadata": {
    "id": "1GWwaVWRo58t"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, num_classes, anchors):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.anchors = anchors\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, predictions, targets, anchors):\n",
    "        # 손실 계산 (objectness, class, bbox)\n",
    "        objectness_loss = self.bce_loss(predictions[..., 0], targets[..., 0])\n",
    "        class_loss = self.ce_loss(predictions[..., 5:], targets[..., 5].long())\n",
    "        box_loss = self.mse_loss(predictions[..., 1:5], targets[..., 1:5])\n",
    "        return objectness_loss + class_loss + box_loss\n"
   ],
   "metadata": {
    "trusted": true,
    "id": "sME-_z6_o58t",
    "ExecuteTime": {
     "end_time": "2024-11-28T01:22:02.486318Z",
     "start_time": "2024-11-28T01:22:02.474032Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Load"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_folder, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): 라벨 정보를 담고 있는 CSV 파일 경로\n",
    "            img_folder (str): 이미지 폴더 경로\n",
    "            transform (callable, optional): 이미지에 적용할 변환 (예: 크기 조정, 데이터 증강)\n",
    "        \"\"\"\n",
    "        self.labels = pd.read_csv(csv_file)  # CSV 파일 읽기\n",
    "        self.img_folder = img_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # CSV에서 이미지 이름 및 YOLO 형식의 데이터 가져오기\n",
    "        image_id = self.labels.iloc[idx]['image_id']\n",
    "        x_center = self.labels.iloc[idx]['x_center']\n",
    "        y_center = self.labels.iloc[idx]['y_center']\n",
    "        width = self.labels.iloc[idx]['width']\n",
    "        height = self.labels.iloc[idx]['height']\n",
    "        class_id = self.labels.iloc[idx]['class_id']\n",
    "\n",
    "        # 바운딩 박스 및 클래스 정보\n",
    "        bbox = torch.tensor([x_center, y_center, width, height], dtype=torch.float32)\n",
    "        label = torch.tensor(class_id, dtype=torch.long)\n",
    "\n",
    "        # 이미지 로드\n",
    "        img_path = os.path.join(self.img_folder, image_id)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # 이미지 변환 적용\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 추가 메타 정보 (필요 시)\n",
    "        disease = self.labels.iloc[idx]['disease']  # 선택적으로 활용 가능\n",
    "\n",
    "        return image, bbox, label\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# 데이터셋 객체 생성\n",
    "dataset = YoloDataset(csv_file=CSV_DIR, img_folder=IMG_FOLDER_DIR, transform=transform)\n",
    "\n",
    "# DataLoader 생성\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = YOLOv3(in_channels=3,num_classes=NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.AdamW (model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "loss_fn = YoloLoss(NUM_CLASSES)\n",
    "scaler = GradScaler()\n",
    "scaled_anchors = (\n",
    "    torch.tensor(ANCHORS)\n",
    "    * torch.tensor(STRIDE).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    ").to(DEVICE)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def training_loop(loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
    "    # Creating a progress bar\n",
    "    progress_bar = tqdm(loader, leave=True)\n",
    "\n",
    "    # Initializing a list to store the losses\n",
    "    losses = []\n",
    "\n",
    "    # Iterating over the training data\n",
    "    for _, (x, y) in enumerate(progress_bar):\n",
    "        x = x.to(DEVICE)\n",
    "        y0, y1, y2 = (\n",
    "            y[0].to(DEVICE),\n",
    "            y[1].to(DEVICE),\n",
    "            y[2].to(DEVICE),\n",
    "        )\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Getting the model predictions\n",
    "            outputs = model(x)\n",
    "            # Calculating the loss at each scale\n",
    "            loss = (\n",
    "                  loss_fn(outputs[0], y0, scaled_anchors[0])\n",
    "                + loss_fn(outputs[1], y1, scaled_anchors[1])\n",
    "                + loss_fn(outputs[2], y2, scaled_anchors[2])\n",
    "            )\n",
    "\n",
    "        # Add the loss to the list\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Optimization step\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Update the scaler for next iteration\n",
    "        scaler.update()\n",
    "\n",
    "        # update progress bar with loss\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        progress_bar.set_postfix(loss=mean_loss)"
   ]
  }
 ]
}
